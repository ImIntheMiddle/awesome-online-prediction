# ğŸ˜ Awesome-Online-Prediction [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of awesome online-prediction papers, tools, and resources.

Created and hosted by the members of group 5 in the 28th **M**eeting on **I**mage **R**ecognition and **U**nderstanding ([MIRU2025](https://cvim.ipsj.or.jp/MIRU2025/index-en.html)) Young Researchers Program.

[ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼ˆMIRU2025ï¼‰](https://cvim.ipsj.or.jp/MIRU2025/index.html)ã§ä¼ç”»ã•ã‚ŒãŸ [è‹¥æ‰‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ](https://sites.google.com/view/miru2025wakate) ã«ãŠã‘ã‚‹ï¼Œã‚°ãƒ«ãƒ¼ãƒ—ï¼•ã«ã‚ˆã‚‹å–ã‚Šçµ„ã¿ã®æˆæœã§ã™ï¼

ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«é–¢ã™ã‚‹é‡è¦ãªè«–æ–‡ï¼Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Œå­¦ç¿’ã®ãŸã‚ã®ãƒªã‚½ãƒ¼ã‚¹ãªã©ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ï¼

> [!TIP]
> Super-awesome ones are marked with a starğŸŒŸ.
> 
> Japanese-only references are marked with JapanğŸ—¾.
> 
> ç‰¹ã«é‡è¦ãƒ»æœ‰ç”¨ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ã«ã¯æ˜Ÿå°ğŸŒŸã‚’ä»˜ã—ã¦ã„ã¾ã™ï¼
> 
> æ—¥æœ¬èªã®ã¿ã®æ–‡çŒ®ã«ã¯æ—¥æœ¬ğŸ—¾ã‚’ä»˜ã—ã¦ã„ã¾ã™ï¼

Progress: ![](https://geps.dev/progress/100)

# ğŸ“‘ Papersï¼ˆè«–æ–‡ï¼‰
## Awesome-Surveysï¼ˆã‚µãƒ¼ãƒ™ã‚¤ï¼‰
### 1. ğŸŒŸ[Hoi _et al._ (Neurocomputing, 2018), Online Learning: A Comprehensive Survey](https://arxiv.org/abs/1802.02871)
  - Published in 2018, this survey has been cited over 1,000 times and broadly covers online learning and online prediction topics.
  - 2018å¹´ã®ç™ºè¡¨ã ãŒï¼Œ1000å›ä»¥ä¸Šå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚µãƒ¼ãƒ™ã‚¤ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®å†…å®¹ã‚’åºƒãå–ã‚Šä¸Šã’ã¦ã„ã‚‹ï¼
### 2. ğŸŒŸ[Foster and Rakhlin (arXiv, 2023), Foundations of Reinforcement Learning and Interactive Decision Making](https://arxiv.org/abs/2312.16730)
  - This lecture note introduces various decision-making problems, including online learning and prediction, and explains the theoretical foundations of online reinforcement learning.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ»äºˆæ¸¬ã‚’å«ã‚ãŸå„ç¨®ã®æ„æ€æ±ºå®šå•é¡Œã«ã¤ã„ã¦ç´¹ä»‹ã—ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã®ç†è«–çš„åŸºç¤ã¾ã§ã‚’è§£èª¬ã—ãŸè¬›ç¾©ãƒãƒ¼ãƒˆï¼
### 3. ğŸŒŸ[Shalev-Shwartz (Foundations and Trends in Machine Learning, 2011), Online Learning and Online Convex Optimization](https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf)
  - This survey provides a comprehensive overview of online learning and online convex optimization.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã«é–¢ã™ã‚‹åŒ…æ‹¬çš„ãªã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ï¼
### 4. [Orabona (arXiv, 2019), A Modern Introduction to Online Learning](https://arxiv.org/abs/1912.13213)
  - Awesome paper that introduces the basic concepts of online learning through a modern view of online convex optimization, covering everything from fundamental concepts to dynamic regret.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®ç¾ä»£çš„è¦–ç‚¹ã‹ã‚‰ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’ç´¹ä»‹ã—ï¼ŒåŸºæœ¬æ¦‚å¿µã‹ã‚‰å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã¾ã§ã‚’åºƒãæ‰±ã£ã¦ã„ã‚‹ï¼

## Awesome-Theoretical Researchï¼ˆåŸºç¤ç ”ç©¶ï¼‰
### 5. [Hannan (Contributions to the Theory of Games, 1957), Approximation to Bayes risk in repeated plays](http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf)
  - Awesome paper that introduces Hannan consistency and provides fundamental approximation results for Bayes risk in repeated play games.
  - åå¾©ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹ãƒ™ã‚¤ã‚ºãƒªã‚¹ã‚¯ã®è¿‘ä¼¼ç†è«–ã‚’æä¾›ã—ï¼ŒHannan consistencyã®æ¦‚å¿µã‚’å°å…¥ã—ãŸåŸºç¤çš„è«–æ–‡ï¼
### 6. ğŸŒŸ[Nesterov (Soviet Mathematics Doklady, 1983), A Meyhod of Solving a Convex Programming Problem with Convergence Rate O(1/kÂ²)](https://hengshuaiyao.github.io/papers/nesterov83.pdf)
  - Awesome paper that introduces the accelerated gradient method achieving O(1/kÂ²) convergence rate for convex optimization, a fundamental breakthrough in optimization theory.
  - å‡¸æœ€é©åŒ–ã§O(1/kÂ²)åæŸç‡ã‚’é”æˆã™ã‚‹åŠ é€Ÿå‹¾é…æ³•ã‚’å°å…¥ã—ï¼Œæœ€é©åŒ–ç†è«–ã«ãŠã‘ã‚‹ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ã‚’èµ·ã“ã—ãŸè«–æ–‡ï¼
### 7. [Littlestone (Machine Learning, 1988), Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm](https://link.springer.com/article/10.1023/A:1022869011914)
  - Awesome paper that introduces the Winnow algorithm for online learning with sparse relevant features, providing mistake bounds independent of irrelevant attributes.
  - ç„¡é–¢ä¿‚ãªå±æ€§ãŒå¤šæ•°å­˜åœ¨ã™ã‚‹ç’°å¢ƒã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹ãªé–¢é€£ç‰¹å¾´é‡ã«ç€ç›®ã—ãŸWinnowã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ï¼Œç„¡é–¢ä¿‚ãªå±æ€§ã®æ•°ã«ä¾å­˜ã—ãªã„èª¤ã‚Šå›æ•°ã®ä¸Šç•Œã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 8. [Herbster and Warmuth (Machine Learning, 1998), Tracking the Best Expert](https://link.springer.com/article/10.1023/A:1007424614876)
  - Awesome paper that addresses tracking the best expert in changing environments, introducing algorithms for shifting regret minimization with optimal bounds.
  - å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã§æœ€è‰¯ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«è¿½å¾“ã—ï¼Œæœ€é©ãªãƒã‚¦ãƒ³ãƒ‰ã§ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’æœ€å°åŒ–ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 9. ğŸŒŸ[Vovk (JCSS, 1998), A Game of Prediction with Expert Advice](https://www.sciencedirect.com/science/article/pii/S0022000097915567)
  - Awesome paper that provides a comprehensive game-theoretic framework for prediction with expert advice, establishing fundamental theoretical foundations.
  - Expert adviceã‹ã‚‰ã®äºˆæ¸¬ã®ãŸã‚ã®ã‚²ãƒ¼ãƒ ç†è«–çš„ãªæ çµ„ã¿ã‚’ä¸ãˆï¼Œç†è«–çš„åŸºç›¤ã‚’ç¢ºç«‹ã—ãŸè«–æ–‡ï¼
### 10. ğŸŒŸ[Kivinen and Warmuth (EuroCOLT1999), Averaging Expert Predictions](https://link.springer.com/chapter/10.1007/3-540-49097-3_13)
  - Awesome paper that analyzes averaging strategies for expert predictions, providing theoretical guarantees for weighted averaging schemes.
  - ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã«ãŠã‘ã‚‹å¹³å‡åŒ–æˆ¦ç•¥ã‚’è§£æã—ï¼Œé‡ã¿ä»˜ãå¹³å‡åŒ–ã®ç†è«–ä¿è¨¼ã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 11. [Kivinen and Warmuth (Machine Learning, 2001), Relative Loss Bounds for Multidimensional Regression Problems](https://link.springer.com/article/10.1023/A:1017938623079)
  - Awesome paper that extends online learning theory from single outputs to multidimensional outputs, establishing the foundation for vector-valued online prediction problems.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç†è«–ã‚’å˜ä¸€å‡ºåŠ›ã‹ã‚‰å¤šæ¬¡å…ƒå‡ºåŠ›ã«æ‹¡å¼µã—ï¼Œãƒ™ã‚¯ãƒˆãƒ«å€¤ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬å•é¡Œã®åŸºç¤ã‚’ç¢ºç«‹ã—ãŸè«–æ–‡ï¼
### 12. ğŸŒŸ[Zinkevich (ICML2003), Online Convex Programming and Generalized Infinitesimal Gradient Ascent](https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf)
  - Awesome paper that introduces online convex optimization framework and proves O(âˆšT) regret bounds for online gradient descent, foundational for modern online learning.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®æ çµ„ã¿ã‚’å°å…¥ã—ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‹¾é…é™ä¸‹æ³•ã®O(âˆšT)ãƒªã‚°ãƒ¬ãƒƒãƒˆå¢ƒç•Œã‚’è¨¼æ˜ã—ãŸï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®åŸºç¤çš„è«–æ–‡ï¼
### 13. ğŸŒŸ[Kalai and Vempara (JCSS, 2005), Efficient algorithms for online decision problems](https://www.sciencedirect.com/science/article/pii/S0022000004001394) 
  - Awesome paper that provides efficient algorithms for online decision problems using follow-the-perturbed-leader approach with polynomial-time guarantees.
  - Follow-the-Perturbed-Leaderæˆ¦ç•¥ã«ã‚ˆã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ±ºå®šå•é¡Œã®åŠ¹ç‡çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ï¼Œå¤šé …å¼æ™‚é–“ä¿è¨¼ã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 14. [Crammer _et al._ (JMLR, 2006), Online Passive-Aggressive Algorithms](https://jmlr.org/papers/v7/crammer06a.html)
  - Awesome paper introducing Passive-Aggressive algorithms for online learning that updates on mistakes while remaining passive on correct predictions.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ãŠã„ã¦ï¼Œèª¤ã‚Šã«å¯¾ã—ã¦ç©æ¥µçš„æ›´æ–°ã‚’è¡Œã„æ­£è§£æ™‚ã¯å—å‹•çš„ã«ç•™ã¾ã‚‹Passive-Aggressiveã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 15. [Hazan _et al._ (Machine Learning, 2007), Logarithmic Regret Algorithms for Online Convex Optimization](https://link.springer.com/article/10.1007/s10994-007-5016-8)
  - Awesome paper that achieves logarithmic regret for online convex optimization using strong convexity assumptions, showing significant improvement of bound.
  - å¼·å‡¸æ€§ã®ä»®å®šã®ã‚‚ã¨ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã«ã‚ˆã‚Šå¯¾æ•°ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã—ï¼Œãƒã‚¦ãƒ³ãƒ‰ã‚’å¤§å¹…ã«æ”¹å–„ã—ãŸè«–æ–‡ï¼
### 16. [Cesa-Bianchi _et al._ (Machine Learning, 2007), Improved second-order bounds for prediction with expert advice](https://arxiv.org/abs/math/0602629)
  - Awesome paper that provides tighter regret bounds for expert advice problems by incorporating variance information, showing that online learning can perform much better than worst-case guarantees.
  - ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå•é¡Œã«ãŠã„ã¦åˆ†æ•£æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã‚ˆã‚Šå³å¯†ãªãƒªã‚°ãƒ¬ãƒƒãƒˆå¢ƒç•Œã‚’ä¸ãˆï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ã‚ˆã£ã¦æœ€æ‚ªã‚±ãƒ¼ã‚¹ä¿è¨¼ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«è‰¯ã„æ€§èƒ½ã‚’ç™ºæ®ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸè«–æ–‡ï¼
### 17. [Crammer _et al._ (EMNLP2009), Multi-Class Confidence Weighted Algorithms](https://aclanthology.org/D09-1052/)
  - Awesome paper that extends confidence-weighted learning to multi-class problems for better online classification performance.
  - ä¿¡é ¼åº¦é‡ã¿å­¦ç¿’ã‚’å¤šã‚¯ãƒ©ã‚¹å•é¡Œã«æ‹¡å¼µã—ï¼Œã‚ˆã‚Šè‰¯ã„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†é¡æ€§èƒ½ã‚’å®Ÿç¾ã—ãŸè«–æ–‡ï¼
### 18. ğŸŒŸ[Crammer _et al._ (NeurIPS2009), Adaptive Regularization of Weight Vectors](https://papers.nips.cc/paper_files/paper/2009/hash/8ebda540cbcc4d7336496819a46a1b68-Abstract.html)
  - Awesome paper that introduces adaptive regularization for weight vectors in online learning, automatically adjusting regularization based on past performance.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ãŠã„ã¦ï¼Œéå»ã®æ€§èƒ½ã«åŸºã¥ã„ã¦é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’é©å¿œçš„ã«æ­£å‰‡åŒ–ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 19. ğŸŒŸ[Duchi _et al._ (JMLR, 2010), Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.org/papers/v12/duchi11a.html)
  - Awesome paper that introduces AdaGrad algorithm, revolutionizing online optimization by automatically adapting learning rates based on historical gradients for improved convergence.
  - éå»ã®å‹¾é…ã«åŸºã¥ã„ã¦å­¦ç¿’ç‡ã‚’è‡ªå‹•èª¿æ•´ã™ã‚‹AdaGradã‚’ææ¡ˆã—ãŸï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ã«ãŠã‘ã‚‹é©å‘½çš„è«–æ–‡ï¼
### 20. [McDonald _et al._ (NAACL HLT2010), Distributed Training Strategies for the Structured Perceptron](https://aclanthology.org/N10-1069.pdf)
  - Awesome paper that develops distributed training strategies for structured perceptron, enabling parallel online learning for structured prediction tasks.
  - æ§‹é€ åŒ–ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã®åˆ†æ•£å­¦ç¿’æˆ¦ç•¥ã‚’é–‹ç™ºã—ï¼Œæ§‹é€ åŒ–äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’å®Ÿç¾ã—ãŸè«–æ–‡ï¼
### 21. [Chu _et al._ (KDD2011), Unbiased online active learning in data streams](https://dl.acm.org/doi/10.1145/2020408.2020444)
  - Awesome paper that addresses unbiased online active learning in data streams, providing theoretical guarantees for selective sampling strategies.
  - ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¯¾ã™ã‚‹ä¸åã‚ªãƒ³ãƒ©ã‚¤ãƒ³èƒ½å‹•å­¦ç¿’ã«å–ã‚Šçµ„ã¿ï¼Œé¸æŠçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ç†è«–ä¿è¨¼ã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 22. [Shalev-Shwartz _et al._ (Mathematical Programming, 2017), Pegasos: primal estimated sub-gradient solver for SVM](https://link.springer.com/article/10.1007/s10107-010-0420-4)
  - Awesome paper that introduces Pegasos algorithm for SVM training with convergence guarantees.
  - SVMï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼‰ã®å­¦ç¿’ã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹Pegasosã‚’ææ¡ˆã—ï¼ŒåæŸä¿è¨¼ã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 23. ğŸŒŸ[Cesa-Bianchi and Lugosi (JCSS, 2012), Combinatorial bandits](https://www.sciencedirect.com/science/article/pii/S0022000012000219)
  - Awesome paper that introduces combinatorial bandits framework, extending multi-armed bandits to combinatorial action spaces with efficient algorithms.
  - çµ„åˆã›è«–çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®æ çµ„ã¿ã‚’å°å…¥ã—ï¼Œå¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚’çµ„åˆã›è¡Œå‹•ç©ºé–“ã«æ‹¡å¼µã—ã¦åŠ¹ç‡çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›ã—ãŸè«–æ–‡ï¼
### 24. ğŸŒŸ[Suehiro _et al._ (ALT2012), Online Prediction under Submodular Constraints](https://api.lib.kyushu-u.ac.jp/opac_download_md/1932327/alt12.pdf)
  - Awesome paper that studies online prediction under submodular constraints, providing regret bounds for constrained online learning problems.
  - åŠ£ãƒ¢ã‚¸ãƒ¥ãƒ©åˆ¶ç´„ä¸‹ã§ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«å–ã‚Šçµ„ã¿ï¼Œåˆ¶ç´„ä»˜ãã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’å•é¡Œã®ãƒªã‚°ãƒ¬ãƒƒãƒˆå¢ƒç•Œã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 25. [Wang _et al._ (ICML2012), Exact Soft Confidence-Weighted Learning](https://arxiv.org/abs/1206.4612)
  - Awesome paper that develops exact soft confidence-weighted learning, providing precise probabilistic updates for online classification with theoretical guarantees.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†é¡ã«ãŠã‘ã‚‹ï¼Œç†è«–ä¿è¨¼ä»˜ãã®ã‚ˆã‚Šå„ªã‚ŒãŸç¢ºç‡çš„æ›´æ–°ã¨ã—ã¦ã€ŒExact Soft Confidence-Weighted Learningã€ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 26. ğŸŒŸ[Bubeck and Slivkins (COLT2012), The best of both worlds: stochastic and adversarial bandits](http://sbubeck.com/COLT12_BS.pdf)
  - Awesome paper that achieves the best of both worlds in bandits, providing algorithms that perform well in both stochastic and adversarial settings simultaneously.
  - ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã§ç¢ºç‡çš„ãƒ»æ•µå¯¾çš„ä¸¡è¨­å®šã§åŒæ™‚ã«æœ€è‰¯æ€§èƒ½ã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 27. [Neu and BartÃ³k (ALT2013), An efficient algorithm for learning with semi-bandit feedback](https://arxiv.org/abs/1305.2732)
  - Awesome paper that provides efficient algorithms for semi-bandit feedback, extending bandit learning to partial information settings with multiple simultaneous actions.
  - ã‚»ãƒŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã™ã‚‹åŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›ã—ï¼Œãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå­¦ç¿’ã‚’è¤‡æ•°åŒæ™‚è¡Œå‹•ã‚’ä¼´ã†éƒ¨åˆ†æƒ…å ±è¨­å®šã«æ‹¡å¼µã—ãŸè«–æ–‡ï¼
### 28. [Gaillard _et al._ (COLT2014), A Second-order Bound with Excess Losses](https://arxiv.org/abs/1402.2044)
  - Awesome paper that derives second-order regret bounds with excess losses, providing tighter analysis for online learning algorithms using variance information.
  - éå‰°æå¤±ã‚’ç”¨ã„ãŸ2æ¬¡ãƒªã‚°ãƒ¬ãƒƒãƒˆå¢ƒç•Œã‚’å°å‡ºã—ï¼Œåˆ†æ•£æƒ…å ±ã‚’æ´»ç”¨ã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚Šå³å¯†ãªè§£æã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 29. ğŸŒŸ[Kingma and Ba (ICLR2015), Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
  - Awesome paper that introduces Adam optimizer combining advantages of AdaGrad and RMSprop, becoming the most widely used adaptive optimization method.
  - AdaGradã¨RMSpropã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ãŸAdamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’å°å…¥ã—ï¼Œæœ€ã‚‚åºƒãä½¿ç”¨ã•ã‚Œã‚‹é©å¿œçš„æœ€é©åŒ–æ‰‹æ³•ã¨ãªã£ãŸè«–æ–‡ï¼
### 30. [Luo and Schapire (COLT2015), Achieving All with No Parameters: AdaNormalHedge](https://proceedings.mlr.press/v40/Luo15.pdf)
  - Awesome paper that proposes AdaNormalHedge, a truly parameter-free algorithm for expert advice that simultaneously achieves multiple objectives without prior information including adaptive regret and unknown competitor performance. 
  - ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã«ãŠã„ã¦ï¼Œäº‹å‰æƒ…å ±ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æ•°ãªã©ï¼‰ã‚’å¿…è¦ã¨ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ãƒªãƒ¼ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ŒAdaNormalHedgeã€ã‚’ææ¡ˆï¼
### 31. [Hazan _et al._ (ICML2017), Efficient Regret Minimization in Non-Convex Games](https://proceedings.mlr.press/v70/hazan17a.html)
  - Awesome paper that studies online algorithms for non-convex loss functions, defining a new measure called "local regret" based on projected gradient magnitudes from time-smoothed losses and proposing algorithms to efficiently minimize it.
  - éå‡¸ãªæå¤±é–¢æ•°ã‚’å¯¾è±¡ã¨ã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç ”ç©¶ï¼æ™‚é–“å¹³æ»‘åŒ–ã—ãŸæå¤±ï¼ˆéå»kå›ã®æå¤±ã‚’å¹³å‡ã—ãŸã‚‚ã®ï¼‰ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹å°„å½±å‹¾é…ã®å¤§ãã•ã‚’åŸºã«ã—ãŸæ–°ã—ã„å°ºåº¦ã€Œå±€æ‰€ãƒªã‚°ãƒ¬ãƒƒãƒˆã€ã‚’å®šç¾©ã—ï¼Œãã‚Œã‚’åŠ¹ç‡çš„ã«æœ€å°åŒ–ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆï¼
  ### 32. ğŸŒŸ[Zheng and Kwok (ICML2017), Follow the Moving Leader in Deep Learning](https://proceedings.mlr.press/v70/zheng17a.html)
  - Awesome paper that proposes Follow the Moving Leader (FTML), a variant of FTRL for deep learning optimization that adapts quickly to changes by weighting recent samples more heavily. 
  - æ·±å±¤å­¦ç¿’ã®æœ€é©åŒ–ã«ãŠã„ã¦ï¼Œæœ€è¿‘ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚ˆã‚Šé‡ãé‡ã¿ä»˜ã‘ã™ã‚‹ã“ã¨ã§å¤‰åŒ–ã«ç´ æ—©ãé©å¿œã™ã‚‹Follow the Moving Leader (FTML) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 33. ğŸŒŸ[Zhang _et al._ (NeurIPS2018), Adaptive Online Learning in Dynamic Environments](https://arxiv.org/abs/1810.10815)
  - Awesome paper that first establishes theoretical lower bounds for dynamic regret against arbitrary comparison sequences and proposes "Ader" which adaptively combines multiple OGD experts with different step sizes using a meta-algorithm.
  - ä»»æ„ã®æ¯”è¼ƒå¯¾è±¡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã®ç†è«–çš„ãªä¸‹é™ã‚’åˆã‚ã¦æç¤ºã—ãŸç ”ç©¶ï¼ç†è«–çš„ãªä¸‹é™ã¨ä¸€èˆ¬çš„ãªOGDã¨ã®å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã«ä¹–é›¢ãŒã‚ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ï¼Œãã®è§£æ±ºç­–ã¨ã—ã¦ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’æŒã¤è¤‡æ•°ã®OGDï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ãƒ¡ã‚¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é©å¿œçš„ã«çµ±åˆã™ã‚‹æ‰‹æ³•ã€ŒAderã€ã‚’ææ¡ˆï¼
### 34. [Finn _et al._ (ICLR2019), Online Meta-Learning](https://arxiv.org/abs/1902.08438)
  - Awesome paper that introduces online meta-learning which merges ideas from meta-learning and online learning, proposing the follow the meta leader algorithm extending MAML with O(log T) regret guarantee.
  - ãƒ¡ã‚¿å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’èåˆã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¡ã‚¿å­¦ç¿’ã‚’å°å…¥ã—ï¼ŒMAMLã‚’æ‹¡å¼µã—ãŸfollow the meta leaderã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’O(log T)ã®regretä¿è¨¼ã¨ã¨ã‚‚ã«ææ¡ˆã—ãŸè«–æ–‡ï¼
### 35. [Zhao _et al._ (NeurIPS2020), Dynamic Regret of Convex and Smooth Functions](https://arxiv.org/abs/2007.03479)
  - Awesome paper that enhances dynamic regret bounds by exploiting smoothness conditions, replacing the dependence on T with problem-dependent quantities like gradient variation and comparator loss, making bounds adaptive to problem difficulty.
  - å‡¸ã‹ã¤æ»‘ã‚‰ã‹ãªé–¢æ•°ã«å¯¾ã™ã‚‹å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’ç ”ç©¶ã—ï¼Œæ»‘ã‚‰ã‹ã•æ¡ä»¶ã‚’æ´»ç”¨ã—ã¦Tã¸ã®ä¾å­˜ã‚’å‹¾é…å¤‰å‹•ã‚„æ¯”è¼ƒå¯¾è±¡ã®æå¤±ãªã©ã®å•é¡Œä¾å­˜é‡ã«ç½®ãæ›ãˆï¼Œå•é¡Œã®é›£æ˜“åº¦ã«é©å¿œçš„ãªå¢ƒç•Œã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 36. [Ito (NeurIPS2021), On Optimal Robustness to Adversarial Corruption in Online Decision Problems](https://arxiv.org/abs/2109.10963)
  - Awesome paper that studies optimal robustness to adversarial corruption in online decision problems, analyzing how online algorithms can maintain performance guarantees when a fraction of inputs are adversarially corrupted.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ±ºå®šå•é¡Œã«ãŠã‘ã‚‹æ•µå¯¾çš„æ‘‚å‹•ã«å¯¾ã™ã‚‹æœ€é©ãªé ‘å¥æ€§ã‚’ç ”ç©¶ã—ï¼Œå…¥åŠ›ã®ä¸€éƒ¨ã«æ•µå¯¾çš„ãªæ‘‚å‹•ãŒåŠ ã‚ã£ãŸå ´åˆã§ã‚‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ€§èƒ½ä¿è¨¼ã‚’ç¶­æŒã™ã‚‹æ–¹æ³•ã‚’è§£æã—ãŸè«–æ–‡ï¼
### 37. ğŸŒŸ[Zimmert and Seldin (JMLR, 2021), Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits](https://arxiv.org/abs/1807.07623)
  - Awesome paper that derives Tsallis-INF algorithm using online mirror descent with Tsallis entropy regularization, achieving optimal regret in both stochastic and adversarial multi-armed bandits.
  - Tsallisã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã‚’ç”¨ã„ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒŸãƒ©ãƒ¼é™ä¸‹æ³•ã«åŸºã¥ãTsallis-INFã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å°å‡ºã—ï¼Œç¢ºç‡çš„ãƒ»æ•µå¯¾çš„å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆä¸¡æ–¹ã§æœ€é©ãªãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 38. ğŸŒŸ[Baby _et al._ (NeurIPS2023), Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms](https://neurips.cc/virtual/2023/poster/71994)
  - Awesome paper that tackles online learning under changing data distributions, developing practical algorithms that automatically adapt to distribution shifts without prior knowledge while achieving optimal theoretical guarantees.
  - ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒå¤‰åŒ–ã™ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç’°å¢ƒã«ãŠã„ã¦ï¼Œåˆ†å¸ƒã‚·ãƒ•ãƒˆã«äº‹å‰çŸ¥è­˜ãªã—ã§è‡ªå‹•é©å¿œã™ã‚‹å®Ÿç”¨çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã—ï¼Œæœ€é©ãªç†è«–ä¿è¨¼ã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 39. [Dai _et al._ (CVPR2025), Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret](https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal_CVPR_2025_paper.html)
  - Awesome paper that addresses label shift in online learning settings by constructing a novel convex risk estimator and enhanced online algorithm, achieving minimax optimal universal dynamic regret.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç’°å¢ƒã§ã®ãƒ©ãƒ™ãƒ«ã‚·ãƒ•ãƒˆå•é¡Œã«å¯¾ã—ã¦æ–°ã—ã„å‡¸ãƒªã‚¹ã‚¯æ¨å®šå™¨ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ§‹ç¯‰ã—ï¼Œminimaxæœ€é©ãªæ±ç”¨å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã—ãŸè«–æ–‡ï¼

## Awesome-Applied Researchï¼ˆå¿œç”¨ç ”ç©¶ï¼‰
### 40. ğŸŒŸ[Bashratat _et al._ (CVPR2008), Learning object motion patterns for anomaly detection and improved object detection](https://ieeexplore.ieee.org/document/4587510)
  - Awesome paper that learns object motion patterns in surveillance videos for anomaly detection and improved object detection.
  - ç›£è¦–æ˜ åƒã«ãŠã‘ã‚‹å¿œç”¨ã¨ã—ã¦ï¼Œç‰©ä½“ã®å‹•ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã—ï¼Œç•°å¸¸æ¤œçŸ¥ã¨ç‰©ä½“æ¤œå‡ºã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 41. [Arora _et al._ (Theory of Computing, 2012), The Multiplicative Weights Update Method: a Meta-Algorithm and Applications](https://theoryofcomputing.org/articles/v008a006/)
  - Awesome paper that presents multiplicative weights update as a meta-algorithm with broad applications across computer science and optimization.
  - ä¹—æ³•çš„é‡ã¿æ›´æ–°ã‚’ãƒ¡ã‚¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦æ‰ãˆï¼Œãã®å¹…åºƒã„å¿œç”¨ã‚’ç¤ºã—ãŸè«–æ–‡ï¼
### 42. [Ho _et al._(NeurIPS2013), More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server](https://fid3024.github.io/papers/2013%20-%20More%20Effective%20Distributed%20ML%20via%20a%20Stale%20Sychronous%20Parallel%20Parameter%20Server.pdf)
  - Awesome paper that introduces Stale Synchronous Parallel Parameter Server for distributed machine learning, improving efficiency through asynchronous updates.
  - åˆ†æ•£æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®ã€ŒStale Synchronous Parallel Parameter Serverã€ã‚’å°å…¥ã—ï¼ŒéåŒæœŸæ›´æ–°ã«ã‚ˆã‚ŠåŠ¹ç‡æ€§ã‚’å‘ä¸Šã•ã›ãŸè«–æ–‡ï¼
### 43. [McMahan _et al._ (KDD2013), Ad Click Prediction: a View from the Trenches](https://research.google/pubs/ad-click-prediction-a-view-from-the-trenches/)
  - Awesome paper that presents practical insights for ad click prediction from large-scale deployment, bridging theory and real-world online learning applications.
  - å¤§è¦æ¨¡ãªãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã«ãŠã‘ã‚‹åºƒå‘Šã‚¯ãƒªãƒƒã‚¯äºˆæ¸¬ã«å–ã‚Šçµ„ã¿ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ç†è«–ã¨å®Ÿä¸–ç•Œã«ãŠã‘ã‚‹å¿œç”¨ã‚’æ©‹æ¸¡ã—ã—ãŸè«–æ–‡ï¼
### 44. [Grnarova _et al._ (ICLR2018), An Online Learning Approach to Generative Adversarial Networks](https://arxiv.org/abs/1706.03269)
  - Awesome paper that applies online learning techniques to GAN training for improved stability.
  - GANã®è¨“ç·´ã«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã‚’é©ç”¨ã—ã¦å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ãŸè«–æ–‡ï¼
### 45. [Song _et al._ (Machine Learning, 2024), No Regret Sample Selection with Noisy Labels](https://arxiv.org/abs/2003.03179)
  - Awesome paper that proposes adaptive k-set selection for training DNNs with noisy labels while providing theoretical regret bounds. 
  - ãƒã‚¤ã‚¸ãƒ¼ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã§ã®DNNè¨“ç·´ã«ãŠã„ã¦ï¼Œç†è«–çš„ãªãƒªã‚°ãƒ¬ãƒƒãƒˆä¿è¨¼ã‚’æŒã¤é©å¿œçš„k-seté¸æŠæ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 46. [Song _et al._ (WACV2020), Adaptive Aggregation of Arbitrary Online Trackers with a Regret Bound](https://openaccess.thecvf.com/content_WACV_2020/papers/Song_Adaptive_Aggregation_of_Arbitrary_Online_Trackers_with_a_Regret_Bound_WACV_2020_paper.pdf)
  - Awesome paper that proposes delayed-Hedge algorithm for aggregating arbitrary online trackers with theoretical regret guarantees in adversarial environments.
  - æ•µå¯¾çš„ç’°å¢ƒã«ãŠã„ã¦ç†è«–çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆä¿è¨¼ã‚’æŒã¤delayed-Hedgeã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ï¼Œä»»æ„ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’é›†ç´„ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 47. [Matsuo _et al._ (ICASSP2023), Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization](https://arxiv.org/abs/2302.08947)
  - Awesome paper that proposes online pseudo-labeling with regret minimization for Learning from Label Proportions, effectively handling large bag sizes.
  - Learning from Label Proportions (LLP) ã«ãŠã„ã¦ï¼Œãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ã«ã‚ˆã‚‹æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ±ºå®šæ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼å¤§ããªbagã‚µã‚¤ã‚ºã§ã‚‚åŠ¹æœçš„ã«å‹•ä½œã™ã‚‹ï¼
### 48. [Å vihrovÃ¡ _et al._ (Frontiers in Digital Health, 2025), Designing digital health interventions with causal inference and multi-armed bandits: a review](https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1435917/full)
  - Awesome paper that reviews how to design digital health interventions using multi-armed bandits and causal inference for Just-In-Time Adaptive Interventions in behavioral change support systems.
  - ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢åˆ†é‡ã«ãŠã‘ã‚‹Just-In-Time Interventionã«å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨å› æœè§£æã‚’å°å…¥ã™ã‚‹æ–¹æ³•è«–ã«é–¢ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼è«–æ–‡ï¼è¢«é¨“è€…ã®å¥åº·çŠ¶æ…‹ã‚’é€æ¬¡çš„ã«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã—ï¼Œé©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§"ä»‹å…¥"ã—ã¦è¡Œå‹•å¤‰å®¹ã‚’ä¿ƒã™æ çµ„ã¿ï¼
### 49. [Kumar _et al._ (AAAI2024), Using adaptive bandit experiments to increase and investigate engagement in mental health](https://ojs.aaai.org/index.php/AAAI/article/view/30328)
  - Awesome paper that presents a software system using Thompson Sampling bandit algorithms for adaptive text-message-based mental health interventions, evaluated with 1100 users to simultaneously improve engagement and collect data for analysis.
  - ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹ã«ãŠã‘ã‚‹å€‹åˆ¥åŒ–åŒ»ç™‚ã«ãŠã„ã¦Thompson Samplingã‚’ç”¨ã„ãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã—ï¼Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨1100äººã®ãƒ¦ãƒ¼ã‚¶ã§å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ã—ãŸè«–æ–‡ï¼
### 50. [GutiÃ©rrez _et al._ (MICCAI2017), A Multi-armed Bandit to Smartly Select a Training Set from Big Medical Data](https://link.springer.com/chapter/10.1007/978-3-319-66179-7_5)
  - Awesome paper that formulates efficient training set selection from large-scale medical imaging data as a multi-armed bandit problem, using clustering-based exploration-exploitation for age prediction from brain images.
  - å¤§è¦æ¨¡ãªåŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é©åˆ‡ã‹ã¤åŠ¹ç‡çš„ã«é¸æŠã™ã‚‹å•é¡Œã‚’å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨ã—ã¦å®šå¼åŒ–ã—ï¼Œè„³ç”»åƒã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ç‰¹å¾´é‡ã‹ã‚‰å¹´é½¢ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œã«å–ã‚Šçµ„ã‚“ã è«–æ–‡ï¼äº‹å‰ã«è¦³æ¸¬ã§ãã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹ã¨ã„ã†å•é¡Œã«è½ã¨ã—è¾¼ã¿ï¼Œãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã¦æœ‰ç›Šãã†ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ´»ç”¨ã—ã¤ã¤ã€ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚‚æ¢ç´¢ã—ã¦ã„ãã€ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã¨ã£ãŸï¼æ‰‹æ³•ã¯ç·šå½¢å›å¸°ãƒ™ãƒ¼ã‚¹ã§æ·±å±¤å­¦ç¿’ã§ã¯ãªã„ï¼
### 51. [Pandian _et al._ (Scientific Reports, 2025), Enhancing lane detection in autonomous vehicles with multi-armed bandit ensemble learning](https://www.nature.com/articles/s41598-025-86743-z)
  - Awesome paper that introduces Multi-Armed Bandit Ensemble (MAB-Ensemble) for lane detection in autonomous vehicles, dynamically selecting optimal CNN models based on environmental conditions using Thompson sampling.
  - è‡ªå‹•é‹è»¢è»Šã®ãƒ¬ãƒ¼ãƒ³æ¤œå‡ºã«ãŠã„ã¦ï¼ŒThompson Samplingã‚’ç”¨ã„ã¦ç’°å¢ƒæ¡ä»¶ã«åŸºã¥ã„ã¦æœ€é©ãªCNNãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„é¸æŠã™ã‚‹Multi-Armed Bandit Ensemble (MAB-Ensemble)ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼

# ğŸ§° Toolsï¼ˆãƒ„ãƒ¼ãƒ«ï¼‰
## Awesome-Librariesï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
### 52. ğŸŒŸ[River](https://github.com/online-ml/river)
  - A Python library for online machine learning (with over 5k stars), covering time series forecasting, bandits, and so on.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ5000ã‚¹ã‚¿ãƒ¼è¶…ãˆï¼‰ã§ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãªã©ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã€‚
### 53. [scikit-multiflow](https://scikit-multiflow.readthedocs.io/en/stable/index.html)
  - A machine learning library for streaming data in Python (~0.8k stars). Although it can handle drift detection and has a variety of algorithms other than neural networks, River is more common nowadays.
  - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸPythonå®Ÿè£…ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆç´„800ã‚¹ã‚¿ãƒ¼ï¼‰ï¼ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ©Ÿèƒ½ã‚’å‚™ãˆï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä»¥å¤–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚‚è±Šå¯Œã ãŒï¼Œç¾åœ¨ã§ã¯Riverã«ä¸»æµãŒç§»ã£ãŸï¼
### 54. ğŸŒŸ[Vowpal Wabbit](https://vowpalwabbit.org/index.html)
  - A machine learning library implemented in various languages (with over 8,500 stars), primarily developed by Microsoft. It supports various learning paradigms, including online learning, and can handle online prediction-related stuff like contextual bandits."
  - MicrosoftãŒä¸­å¿ƒã¨ãªã£ã¦é–‹ç™ºã—ã¦ã„ã‚‹å¤šè¨€èªå®Ÿè£…ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ8500ã‚¹ã‚¿ãƒ¼è¶…ãˆï¼‰ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’å«ã‚€å¤šæ§˜ãªå­¦ç¿’æ§˜å¼ã«å¯¾å¿œã—ã¦ãŠã‚Šï¼ŒContextual banditsç­‰ã‚’æ‰±ãˆã‚‹ï¼
### 55. [MOA](https://moa.cms.waikato.ac.nz/)
  - An open-source Java framework designed for sequential data processing, boasting over 600 stars.
  - JAVAã§å®Ÿè£…ã•ã‚ŒãŸï¼Œé€æ¬¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼600ã‚¹ã‚¿ãƒ¼è¶…ãˆï¼ 
### 56. [CapyMOA](https://capymoa.org/)
  - Python implementation of MOA, significantly faster than River and suited for real-time processing.
  - MOAã®Pythonç‰ˆï¼Riverã‚ˆã‚Šã‚‚å¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¦ãŠã‚Šï¼Œãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å‘ãï¼
### 57. [Jubatas](http://jubat.us/ja/index.html)
  - A distributed processing framework for online machine learning, jointly developed by PFN and NTT.
  - PFNã¨NTTãŒå…±åŒã§é–‹ç™ºã—ã¦ã„ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’å‘ã‘ã®åˆ†æ•£å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼
### 58. [Deep-River](https://online-ml.github.io/deep-river/)
  - A library suitable for online learning of deep learning models implemented in PyTorch. Same developers as River (online-ml)."
  - PyTorchã§å®Ÿè£…ã•ã‚ŒãŸæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«é©ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Riverã¨åŒã˜online-mlãŒé–‹ç™ºï¼

## Awesome-Probability Inequalities (ç¢ºç‡ä¸ç­‰å¼)
### 59. [Probability inequalities](https://probability.oer.math.uconn.edu/wp-content/uploads/sites/2187/2020/08/ch15M.pdf)
  - Introduction of several probability inequalities used in proofs.
  - è¨¼æ˜ã«ä½¿ç”¨ã•ã‚Œã‚‹ç¢ºç‡ä¸ç­‰å¼ãŒã„ãã¤ã‹ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ï¼
### 60. ğŸŒŸ[Markov's Inequality ... Made Easy!](https://www.youtube.com/watch?v=e-nAr3MkAII)
  - Awesome YouTube video on Markov's inequality.
  - ãƒãƒ«ã‚³ãƒ•ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 61. ğŸŒŸ[Chebyshev's Inequality ... Made Easy!](https://www.youtube.com/watch?v=mlelI1LA9o4)
  - Awesome YouTube video on Chebyshev's inequality.
  - ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 62. ğŸ—¾[ã€å¤§å­¦æ•°å­¦ã€‘ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã€ç¢ºç‡çµ±è¨ˆã€‘](https://www.youtube.com/watch?v=d-ugoDdXWrU)
  - ãƒ¨ãƒ“ãƒãƒªã«ã‚ˆã‚‹ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã«ã¤ã„ã¦ã®è§£èª¬å‹•ç”»ï¼
### 63. [What is the Chernoff Bound?](https://www.youtube.com/watch?v=WKUeBoQp2Uo)
  - Awesome YouTube video on Chernoff bound.
  - ãƒã‚§ãƒ«ãƒãƒ•é™ç•Œã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 64. [L 27 | Cauchy Schwarz Inequality | Probability & Statistics | Digital Communication](https://www.youtube.com/watch?v=14-JD5KiUz0)
  - Awesome YouTube video on Cauchy-Schwarz inequality.
  - ã‚³ãƒ¼ã‚·ãƒ¼ï¼ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
  - ãƒã‚§ãƒ«ãƒãƒ•é™ç•Œã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 65. [Jensen's Inequality](https://www.youtube.com/watch?v=u0_X2hX6DWE)
  - Awesome YouTube video on Jensen's inequality.
  - ã‚¤ã‚§ãƒ³ã‚»ãƒ³ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 66. [A Visual Introduction to Hoeffding's Inequality - Statistical Learning Theory](https://www.youtube.com/watch?v=lsYPC0MuLJA)
  - Awesome YouTube video visualizing the concept of Hoeffding's inequality.
  - ã¸ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è¦–è¦šçš„ã«è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 67. [Supplemental Lecture notes Hoeffdingâ€™s inequality](https://cs229.stanford.edu/extra-notes/hoeffding.pdf)
  - Lecture material of Stanford University, including an explanation of moment generating functions and a proof of Hoeffding's inequality.
  - ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ã®è§£èª¬ã‚„Hoeffdingã®ä¸ç­‰å¼ã®è¨¼æ˜ã‚’å«ã‚“ã ï¼Œã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ã®è¬›ç¾©è³‡æ–™ï¼
### 68. ğŸ—¾[ãƒ˜ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ç­‰å¼(Hoeffding's inequality)ã¨è«¸ã€…ã®ç¢ºç‡ã®è©•ä¾¡ã®ä¸ç­‰å¼](https://ludu-vorton.hatenablog.com/entry/2019/06/06/073000)
  - çµ±è¨ˆçš„å­¦ç¿’ç†è«–ã§ç¢ºç‡ã®è©•ä¾¡ã§ç”¨ã„ã‚‰ã‚Œã‚‹æ§˜ã€…ãªä¸ç­‰å¼ï¼ˆã¸ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ç­‰å¼ã‚’å«ã‚€ï¼‰ã«ã¤ã„ã¦ã®è§£èª¬ï¼

## Awesome-Convex Optimization (å‡¸æœ€é©åŒ–)
### 69. [Subgradients/Subderivatives - Convex Analysis](https://www.youtube.com/watch?v=o0rOaN5uo64)
  - Awesome YouTube video on subgradients and subderivatives.
  - åŠ£å‹¾é…/åŠ£å¾®åˆ†ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 70. [Lipschitz Continuity | Lipschitz Condition](https://www.youtube.com/watch?v=P-OFTp3BPis) 
  - Awesome YouTube video on Lipschitz continuity.
  - ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 71. ğŸ—¾[ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã¨ã¯ï½å®šç¾©ã¨æ€§è³ªãƒ»ä»–ã®é€£ç¶šæ€§ã¨ã®é–¢ä¿‚ãªã©ï½](https://mathlandscape.com/lipschitz/)
  - ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã®å®šç¾©ã‚„ä¾‹ï¼Œæ€§è³ªï¼Œãã®ä»–ã®é€£ç¶šæ€§ã¨ã®é–¢é€£æ€§ã«ã¤ã„ã¦è§£èª¬ã—ãŸè¨˜äº‹ï¼
### 72. [Lagrange Multipliers](https://www.youtube.com/watch?v=5-CUqogfPLY)
  - Awesome YouTube video on Lagrange multipliers.
  - ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 73. [Understanding Lagrange Multipliers Visually](https://www.youtube.com/watch?v=5A39Ht9Wcu0)
  - Awesome YouTube video visualizing the concept of Lagrange multipliers.
  - ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦è¦–è¦šçš„ã«è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 74. ğŸ—¾[ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã®æ°—æŒã¡ã€æ¡ä»¶ä»˜ãæ¥µå€¤å•é¡Œã€‘](https://www.youtube.com/watch?v=vAwqZmwf4W8)
  - ãƒ¨ãƒ“ãƒãƒªã«ã‚ˆã‚‹ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦ã®è§£èª¬å‹•ç”»ï¼å›³å½¢çš„æ„å‘³ã«ã¤ã„ã¦ã®è§£èª¬ã‚’å«ã‚€ï¼
### 75. ğŸ—¾[åˆ¶ç´„ä»˜ãæœ€é©åŒ–å•é¡Œ(KKTæ¡ä»¶/ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•)](https://www.youtube.com/watch?v=bdWTCq98H5c)
  - ãƒ¨ãƒ“ãƒãƒªã«ã‚ˆã‚‹ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦ã®è§£èª¬å‹•ç”»ï¼KKTæ¡ä»¶ï¼ˆä¸ç­‰å¼åˆ¶ç´„ã®å ´åˆã®è§£æ³•ï¼‰ã«ã¤ã„ã¦ã®è§£èª¬ã‚’å«ã‚€ï¼
### 76. ğŸ—¾[ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã¨ã¯ï½æ„å‘³ã¨è¨¼æ˜ï½](https://mathlandscape.com/lagrange-multiplier/)
  - ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã®æ„å‘³ï¼Œå®šç†ã¨ãã®è¨¼æ˜ã‚’è§£èª¬ã—ãŸè¨˜äº‹ï¼

## Awesome-Gradient Descent (å‹¾é…é™ä¸‹æ³•)
### 77. ğŸŒŸ[Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=zRN94_SPD4hrQUUI)
  - Awesome explanation of gradient descent in deep learning by 3Blue1Brown.
  - 3Blue1Brownã«ã‚ˆã‚‹ï¼Œæ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹å‹¾é…é™ä¸‹æ³•ã«ã¤ã„ã¦ã®è§£èª¬ï¼
### 78. [Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)](https://youtu.be/NE88eqLngkg?si=qSmU5hpaeYiUtEZw)
  - Awesome explanation of the various online learning methods used in deep learning.
  - æ·±å±¤å­¦ç¿’ã«ç”¨ã„ã‚‰ã‚Œã‚‹æ§˜ã€…ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã«ã¤ã„ã¦ã®è§£èª¬ï¼
### 79. [Mini Batch Gradient Descent (C2W2L01)](https://www.youtube.com/watch?v=4qJaSmvhxi8)
  - Awesome YouTube video by Andrew Ng explaining mini-batch gradient descent.
  - Andrew Ngã«ã‚ˆã‚‹ï¼ŒãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”».
### 80. [Understanding Mini-Batch Gradient Descent (C2W2L02)](https://www.youtube.com/watch?v=-_4Zi8fCZO4)
  - The second YouTube video by Andrew Ng explaining mini-batch gradient descent.
  - Andrew Ngã«ã‚ˆã‚‹ï¼ŒãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ã®ï¼’æœ¬ç›®.

# ğŸ“š Resourcesï¼ˆå­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ï¼‰
## Awesome-slidesï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰
### 81. ğŸŒŸ[Online Convex Optimization and Its Surprising Applications](https://groups.oist.jp/sites/default/files/imce/u129210/mlss/Lecture_slide/MLSS2024_Francesco_Orabona.pdf)
  - Awesome slides from MLSS2024 by Orabona on online convex optimization algorithms such as OGD and OMD, and their applications to other fields. The content is quite mathematical but beneficial.
  - MLSS2024ã«ãŠã‘ã‚‹ï¼ŒOrabonaã«ã‚ˆã‚‹OGDã‚„OMDã¨ã„ã£ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ãã®ä»–åˆ†é‡ã¸ã®å¿œç”¨ã«ã¤ã„ã¦ã®ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ã‹ãªã‚Šæ•°å­¦çš„ãªå†…å®¹ã ãŒæœ‰ç›Šï¼
### 82. [Online Learning Methods for Big Data Analytics](http://www.mysmu.edu.sg/faculty/chhoi/libol/icdm14tuto/index.html)
  - Awesome tutorial presented at IEEE ICDM2014.
  - IEEE ICDM2014ã§ç™ºè¡¨ã•ã‚ŒãŸãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¬›æ¼”ï¼
### 83. [Learning Methods for Online Prediction Problems](https://users.cecs.anu.edu.au/~ssanner/MLSS2010/Bartlett1.pdf)
  - Awesome lecture materials from UC Berkeley covering topics from the expert aggregation problem to online convex optimization, with applications such as portfolio optimization.
  - UC Berkeleyã«ãŠã‘ã‚‹è¬›ç¾©è³‡æ–™ï¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã‹ã‚‰ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ï¼Œå¿œç”¨ã¨ã—ã¦ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã¾ã§ã‚’æ‰±ã£ã¦ã„ã‚‹ï¼
### 84. [Follow the Leader: Theory and Applications](https://www.cs.ubc.ca/labs/lci/mlrg/slides/2019_summer_3_follow_the_leader.pdf)
  - Slides explaining Follow The Leader (FTL) and its derivative algorithms in online learning, along with their applications.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ãŠã‘ã‚‹Follow The Leader (FTL) ã‚„ãã®æ´¾ç”Ÿã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼Œãã—ã¦ãã‚Œã‚‰ã®å¿œç”¨ã«ã¤ã„ã¦è§£èª¬ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ï¼
### 85. ğŸ—¾[å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ç†è«–ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ](https://ibisml.org/archive/ibis2014/ibis2014_bandit.pdf)
  - ç¢ºç‡çš„ï¼ŒãŠã‚ˆã³æ•µå¯¾çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«ãŠã‘ã‚‹ï¼Œå ±é…¬æœ€å¤§åŒ–ï¼ˆãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ï¼‰ã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è§£èª¬ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ï¼
### 86.ğŸ—¾[ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®ç†è«–ã¨å¿œç”¨](https://www.lab2.kuis.kyoto-u.ac.jp/keisan-genkai/reports/2005/zentai_1/04-takimoto.pdf)
  - æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€ï¼ˆå¾Œè¿°ï¼‰ã®è‘—è€…ã§ã‚‚ã‚ã‚‹ç€§æœ¬è‹±äºŒå…ˆç”Ÿã«ã‚ˆã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ãŠã‘ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã¨ãã®å¿œç”¨ä¾‹ã‚’å–ã‚Šä¸Šã’ã¦ã„ã‚‹ï¼

## Awesome-Textbooksï¼ˆæ›¸ç±ï¼‰
### 87. ğŸŒŸ[Prediction, Learning, and Games](https://www.cambridge.org/core/books/prediction-learning-and-games/A05C9F6ABC752FAB8954C885D0065C8F)
  - The bible on online learning, focusing on regret minimization and game-theoretic approaches to sequential decision-making
  - ãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ã‚„ã‚²ãƒ¼ãƒ ç†è«–ã‚’é€šã˜ã¦é€æ¬¡æ„æ€æ±ºå®šå•é¡Œã‚’æ‰±ã†ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ãƒã‚¤ãƒ–ãƒ«çš„æ›¸ç±ï¼
### 88. [Introduction to Online Convex Optimization](https://sites.google.com/view/intro-oco/)
  - Awesome book by Hazan that covers a wide range of topics in the theory of online convex optimization.
  - Hazanã«ã‚ˆã‚‹ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®ç†è«–ã«é–¢ã™ã‚‹å¤šæ§˜ãªãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒãƒ¼ã—ãŸå…¥é–€æ›¸ï¼
### 89. ğŸŒŸğŸ—¾[æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚º ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€](https://www.kspub.co.jp/book/detail/1529229.html)
  - è‘—åãªæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã‚ˆã‚Šï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ã¦æ›¸ã‹ã‚ŒãŸä¸€å†Šï¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ãªã©ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®ä¸»è¦ãªå†…å®¹ã‚’ä¸€é€šã‚Šå­¦ã¹ã‚‹ï¼
### 90. ğŸ—¾[æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚º ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã€](https://www.kspub.co.jp/book/detail/1529038.html)
  - åŒã˜ãæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã‚ˆã‚Šï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã‚’å–ã‚Šä¸Šã’ãŸä¸€å†Šï¼ˆå‡ºç‰ˆã¯ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€ã‚ˆã‚Šã‚‚å‰ï¼‰ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã‚’å«ã‚€ï¼Œã‚ˆã‚Šåºƒç¯„ãªå†…å®¹ã‚’å­¦ã¹ã‚‹ï¼ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€ã‚ˆã‚Šã‚‚å¹³æ˜“ï¼
### 91. ğŸ—¾[æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚º ã€Œãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ç†è«–ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€](https://www.kspub.co.jp/book/detail/1529175.html)
  - åŒã˜ãæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã‚ˆã‚Šï¼ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«ãŠã‘ã‚‹ãƒªã‚°ãƒ¬ãƒƒãƒˆè§£æã‚„å¿œç”¨ä¾‹ãªã©ã‚’ã‚ˆã‚Šå°‚é–€çš„ã«æ‰±ã£ã¦ã„ã‚‹ï¼

## Videosï¼ˆå‹•ç”»ï¼‰
### 92. ğŸŒŸ[An introduction to regret analysis: environment models and best-of-both-worlds](https://youtu.be/pCER8iuTdR4?si=XAL6lP5tj0mMf8yp)
  - Awesome introduction on online learning, regret analysis, and best-of-both-worlds algorithms in the Machine Learning Summer School 2024.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼Œãƒªã‚°ãƒ¬ãƒƒãƒˆè§£æï¼ŒBest-of-both-worldsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦ã®Machine Learning Summer School 2024ã§ã®è¬›æ¼”ï¼
### 93. [Predict with online prediction in Vertex AI](https://youtu.be/TEE7uUbXWDY?si=nHXMfZyTb13KpmWD)
  - Awesome tutorial on how to make predictions on tabular datasets with online prediction in Vertex AI. [GitHub Link](https://github.com/rafaello9472/c4ds/tree/main/Predict%20with%20online%20prediction%20in%20Vertex%20AI)
  - Googleã®Vertex AIã‚’ç”¨ã„ã¦è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã‚’è¡Œã†æ–¹æ³•ã«ã¤ã„ã¦ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼[GitHub Link](https://github.com/rafaello9472/c4ds/tree/main/Predict%20with%20online%20prediction%20in%20Vertex%20AI)
### 94. ["Online" prediction vs "batch" prediction in machine learning](https://youtu.be/DnmWTIeQ7PM?si=Mg8xcbWXyzlP3vLY)
  - Awesome explanation by Chip Huyen on the difference between online prediction and more common batch prediction and their applications.
  - Chip Huyenã«ã‚ˆã‚‹ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã¨é¦´æŸ“ã¿æ·±ã„ãƒãƒƒãƒäºˆæ¸¬ã¨ã®é•ã„ã‚„ãã‚Œã‚‰ã®å¿œç”¨ä¾‹ã«ã¤ã„ã¦ã®çŸ­ã„è§£èª¬ï¼
### 95. [ML Drift: Identifying Issues Before You Have a Problem](https://youtu.be/uOG685WFO00?si=7_ti70FDTD-B5tbO)
  - Awesome presentation by Amy Hodler on ML drifts and how to fix them. [Blog](https://www.fiddler.ai/blog/drift-in-machine-learning-how-to-identify-issues-before-you-have-a-problem)
  - Amy Hodlerã«ã‚ˆã‚‹ï¼ŒMLãƒ‰ãƒªãƒ•ãƒˆã‚„ãã®è§£æ±ºæ–¹æ³•ã«ã¤ã„ã¦ã®ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼[ãƒ–ãƒ­ã‚°](https://www.fiddler.ai/blog/drift-in-machine-learning-how-to-identify-issues-before-you-have-a-problem)

## Articlesï¼ˆè¨˜äº‹ï¼‰
### 96. [What is Online Machine Learning?](https://medium.com/value-stream-design/online-machine-learning-515556ff72c5)
  - A blog post explaining the concept of online machine learning.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã«ã¤ã„ã¦è§£èª¬ã—ãŸãƒ–ãƒ­ã‚°è¨˜äº‹ï¼
### 97. [Anomalies detection using River](https://medium.com/spikelab/anomalies-detection-using-river-398544d3536)
  - A blog post explaining anomaly detection using River, including practical code examples.
  - Riverã‚’ç”¨ã„ãŸç•°å¸¸æ¤œçŸ¥ã«ã¤ã„ã¦ï¼Œå®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã‚’äº¤ãˆã¦è§£èª¬ã—ãŸãƒ–ãƒ­ã‚°è¨˜äº‹ï¼
### 98. ğŸŒŸğŸ—¾[ç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã€](https://www.ai-gakkai.or.jp/resource/my-bookmark/my-bookmark_vol30-no5/)
  - äººå·¥çŸ¥èƒ½å­¦ä¼šèªŒã®é€£è¼‰ã€Œç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’ç‰¹é›†ã—ãŸéš›ã®è¨˜äº‹ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤ã‚„ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼Œé–¢é€£å­¦ä¼šï¼Œé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã¾ã¨ã‚ã¦ã„ã‚‹ï¼
### 99. ğŸ—¾[ç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€Œå¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã€](https://www.ai-gakkai.or.jp/resource/my-bookmark/my-bookmark_vol31-no5/)
  - åŒã˜ãã€Œç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€ã‚ˆã‚Šï¼Œå¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã‚’ç‰¹é›†ã—ãŸå›ã®è¨˜äº‹ï¼é€æ¬¡çš„ãªæ„æ€æ±ºå®šã§ã‚ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«é–¢ã™ã‚‹ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚„é–¢é€£å­¦ä¼šï¼Œé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Œé‡è¦è«–æ–‡ã‚’ã¾ã¨ã‚ã¦ã„ã‚‹ï¼
### 100. [Deus Ex Machinaã€ŒOnline learning and online predictionï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ã¤ã„ã¦ï¼‰ã€](https://deus-ex-machina-ism.com/?p=17082)
  - Good for understanding the confusing differences between online learning and online prediction, and for gaining an overview of their respective scopes.
  - æ··åŒã—ã‚„ã™ã„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®é•ã„ã‚’çŸ¥ã‚Šï¼Œãã‚Œãã‚Œã®ç¯„å›²ã‚’æ¦‚è¦³ã™ã‚‹ã®ã«è‰¯ã„ï¼
### 101. [Deus Ex Machinaã€ŒOverview of online forecasting technology and various applications and implementationsï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬æŠ€è¡“ã®æ¦‚è¦ã¨æ§˜ã€…ãªé©ç”¨äº‹ä¾‹ã¨å®Ÿè£…ä¾‹ï¼‰ã€](https://deus-ex-machina-ism.com/?p=53594)
  - It introduces algorithms, libraries, applications, and suitable books for learning used in online prediction.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Œå¿œç”¨ä¾‹ï¼Œå­¦ç¿’ã«é©ã—ãŸæ›¸ç±ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ï¼
