# ğŸ˜ Awesome-Online-Prediction [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of awesome online-prediction papers, tools, and resources.

Created and hosted by the members of group 5 in the 28th **M**eeting on **I**mage **R**ecognition and **U**nderstanding ([MIRU2025](https://cvim.ipsj.or.jp/MIRU2025/index-en.html)) Young Researchers Program.

[ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼ˆMIRU2025ï¼‰](https://cvim.ipsj.or.jp/MIRU2025/index.html)ã§ä¼ç”»ã•ã‚ŒãŸ [è‹¥æ‰‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ](https://sites.google.com/view/miru2025wakate) ã«ãŠã‘ã‚‹ï¼Œã‚°ãƒ«ãƒ¼ãƒ—ï¼•ã«ã‚ˆã‚‹å–ã‚Šçµ„ã¿ã®æˆæœã§ã™ï¼

ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«é–¢ã™ã‚‹é‡è¦ãªè«–æ–‡ï¼Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Œå­¦ç¿’ã®ãŸã‚ã®ãƒªã‚½ãƒ¼ã‚¹ãªã©ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ï¼

> [!TIP]
> Super-awesome ones are marked with a starğŸŒŸ.
> 
> Japanese-only references are marked with JapanğŸ—¾.
> 
> ç‰¹ã«é‡è¦ãƒ»æœ‰ç”¨ã¨æ€ã‚ã‚Œã‚‹ã‚‚ã®ã«ã¯æ˜Ÿå°ğŸŒŸã‚’ä»˜ã—ã¦ã„ã¾ã™ï¼
> 
> æ—¥æœ¬èªã®ã¿ã®æ–‡çŒ®ã«ã¯æ—¥æœ¬ğŸ—¾ã‚’ä»˜ã—ã¦ã„ã¾ã™ï¼

Progress: ![](https://geps.dev/progress/100)

# ğŸ“‘ Papersï¼ˆè«–æ–‡ï¼‰
## Awesome-Surveysï¼ˆã‚µãƒ¼ãƒ™ã‚¤ï¼‰
### 1. ğŸŒŸ[Hoi _et al._ (Neurocomputing, 2018), Online Learning: A Comprehensive Survey](https://arxiv.org/abs/1802.02871)
  - Published in 2018, this survey has been cited over 1,000 times and broadly covers online learning and online prediction topics.
  - 2018å¹´ã®ç™ºè¡¨ã ãŒï¼Œ1000å›ä»¥ä¸Šå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚µãƒ¼ãƒ™ã‚¤ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®å†…å®¹ã‚’åºƒãå–ã‚Šä¸Šã’ã¦ã„ã‚‹ï¼
### 2. ğŸŒŸ[Foster and Rakhlin (arXiv, 2023), Foundations of Reinforcement Learning and Interactive Decision Making](https://arxiv.org/abs/2312.16730)
  - This lecture note introduces various decision-making problems, including online learning and prediction, and explains the theoretical foundations of online reinforcement learning.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ»äºˆæ¸¬ã‚’å«ã‚ãŸå„ç¨®ã®æ„æ€æ±ºå®šå•é¡Œã«ã¤ã„ã¦ç´¹ä»‹ã—ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã®ç†è«–çš„åŸºç¤ã¾ã§ã‚’è§£èª¬ã—ãŸè¬›ç¾©ãƒãƒ¼ãƒˆï¼
### 3. ğŸŒŸ[Shalev-Shwartz (Foundations and Trends in Machine Learning, 2011), Online Learning and Online Convex Optimization](https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf)
  - This paper provides a foundational and comprehensive survey of the theory of online learning, with a special focus on the framework of online convex optimization, which has become a cornerstone of the field. It serves as a key reference for understanding how various online learning algorithms can be unified and analyzed through the central concept of convexity.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ç†è«–ã«é–¢ã™ã‚‹åŸºç¤çš„ã‹ã¤åŒ…æ‹¬çš„ãªæ¦‚è¦³ã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šï¼Œç‰¹ã«ã“ã®åˆ†é‡ã®åŸºç¤ã¨ãªã£ã¦ã„ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ï¼æ§˜ã€…ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒï¼Œå‡¸æ€§ã¨ã„ã†ä¸­å¿ƒçš„ãªæ¦‚å¿µã‚’é€šã˜ã¦ã„ã‹ã«çµ±åˆã•ã‚Œï¼Œåˆ†æã•ã‚Œã†ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®é‡è¦ãªå‚è€ƒæ–‡çŒ®ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã‚‹ï¼
### 4. [Orabona (arXiv, 2019), A Modern Introduction to Online Learning](https://arxiv.org/abs/1912.13213)
  - This textbook introduces the fundamental concepts and algorithms of online learning from the perspective of online convex optimization, without requiring prior specialized knowledge.
  - å°‚é–€çš„ãªäºˆå‚™çŸ¥è­˜ã‚’å¿…è¦ã¨ã›ãšã«ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®è¦³ç‚¹ã‹ã‚‰ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç´¹ä»‹ã™ã‚‹æ•™ç§‘æ›¸ï¼

## Awesome-Theoretical Researchï¼ˆåŸºç¤ç ”ç©¶ï¼‰
### 5. [Hannan (Contributions to the Theory of Games, 1957), Approximation to Bayes risk in repeated plays](http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf)
  -   This paper provides a foundational framework for sequential decision-making under uncertainty by introducing a strategy for repeated games. Its key contribution is demonstrating that the regretâ€”the difference between the cumulative loss of the sequential strategy and that of the best single fixed strategy chosen in hindsightâ€”is provably bounded and grows at a sublinear rate of $O(\sqrt{N})$, where $N$ is the number of repetitions. This work established a formal basis for regret minimization, a central concept in online prediction, and provided one of the first concrete algorithms with a performance guarantee against an arbitrary sequence of outcomes.
  - ã“ã®è«–æ–‡ã¯ï¼Œç¹°ã‚Šè¿”ã—ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹æˆ¦ç•¥ã‚’å°å…¥ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œä¸ç¢ºå®Ÿæ€§ã®ä¸‹ã§ã®é€æ¬¡æ„æ€æ±ºå®šã®åŸºç¤çš„ãªæ çµ„ã¿ã‚’æä¾›ã—ãŸï¼ãã®ä¸­å¿ƒçš„ãªè²¢çŒ®ã¯ï¼Œãƒªã‚°ãƒ¬ãƒƒãƒˆï¼ˆé€æ¬¡æˆ¦ç•¥ã®ç´¯ç©æå¤±ã¨ï¼Œå¾Œã‹ã‚‰è¦‹ã¦æœ€å–„ã§ã‚ã£ãŸå˜ä¸€ã®å›ºå®šæˆ¦ç•¥ã®æå¤±ã¨ã®å·®ï¼‰ãŒï¼Œè¨¼æ˜å¯èƒ½ã«æœ‰ç•Œã§ã‚ã‚Šï¼Œç¹°ã‚Šè¿”ã—å›æ•°ã‚’ $N$ ã¨ã—ã¦ $O(\sqrt{N})$ ã¨ã„ã†æº–ç·šå½¢ãªãƒ¬ãƒ¼ãƒˆã§å¢—å¤§ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸç‚¹ã«ã‚ã‚‹ï¼ã“ã®ç ”ç©¶ã¯ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®ä¸­å¿ƒæ¦‚å¿µã§ã‚ã‚‹ãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ã®å½¢å¼çš„ãªåŸºç¤ã‚’ç¯‰ãï¼Œä»»æ„ã®çµæœç³»åˆ—ã«å¯¾ã™ã‚‹æ€§èƒ½ä¿è¨¼ã‚’æŒã¤æœ€åˆã®å…·ä½“çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸€ã¤ã‚’æç¤ºã—ãŸï¼
### 6. ğŸŒŸ[Nesterov (Soviet Mathematics Doklady, 1983), A Meyhod of Solving a Convex Programming Problem with Convergence Rate O(1/kÂ²)](https://hengshuaiyao.github.io/papers/nesterov83.pdf)
  - This paper introduces what is now known as Nesterov's Accelerated Gradient (NAG) method, a landmark achievement in convex optimization. Its role in online prediction is a fundamental building block for creating highly efficient algorithms. By providing a method that achieves a faster convergence rate of $O(1/k^2)$ for smooth convex functions, it enables online learning algorithms to adapt much more rapidly and achieve better performance (i.e., lower regret), forming the theoretical basis for many state-of-the-art methods in online convex optimization.
  - ã“ã®è«–æ–‡ã¯ï¼Œç¾åœ¨ãƒã‚¹ãƒ†ãƒ­ãƒ•ã®åŠ é€Ÿå‹¾é…æ³•ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã‚‹å‡¸æœ€é©åŒ–ã«ãŠã‘ã‚‹ç”»æœŸçš„ãªæˆæœã‚’å°å…¥ã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ãŠã‘ã‚‹ã“ã®è«–æ–‡ã®å½¹å‰²ã¯ï¼Œéå¸¸ã«åŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªæ§‹æˆè¦ç´ ã§ã‚ã‚‹ï¼å¹³æ»‘ãªå‡¸é–¢æ•°ã«å¯¾ã—ã¦ $O(1/k^2)$ ã¨ã„ã†é€Ÿã„åæŸç‡ã‚’é”æˆã™ã‚‹æ‰‹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã‚ˆã‚Šè¿…é€Ÿã«é©å¿œã—ï¼Œã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆã™ãªã‚ã¡ï¼Œã‚ˆã‚Šä½ã„ãƒªã‚°ãƒ¬ãƒƒãƒˆï¼‰ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã«ãŠã‘ã‚‹å¤šãã®æœ€å…ˆç«¯æ‰‹æ³•ã®ç†è«–çš„åŸºç¤ã‚’å½¢æˆã—ã¦ã„ã‚‹ï¼
### 7. [Littlestone (Machine Learning, 1988), Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm](https://link.springer.com/article/10.1023/A:1022869011914)
  - This paper introduces the Winnow algorithm, which is a seminal contribution to online learning. Its primary importance lies in providing a mistake-bound model that is highly effective in high-dimensional settings where many attributes are irrelevant. The number of mistakes Winnow makes grows only logarithmically with the number of irrelevant attributes, making it significantly more efficient than previous algorithms like the Perceptron in such scenarios. This established a foundational approach for handling sparse target concepts efficiently within the online learning framework.
  - ã“ã®è«–æ–‡ã¯ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¸ã®ç‹¬å‰µçš„ãªè²¢çŒ®ã§ã‚ã‚‹ Winnow ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å°å…¥ã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼ãã®ä¸»ãªé‡è¦æ€§ã¯ï¼Œå¤šãã®å±æ€§ãŒç„¡é–¢ä¿‚ã§ã‚ã‚‹é«˜æ¬¡å…ƒã®çŠ¶æ³ã«ãŠã„ã¦éå¸¸ã«åŠ¹æœçš„ãªèª¤ã‚Šå›æ•°ä¿è¨¼ä»˜ãã®ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ãŸç‚¹ã«ã‚ã‚‹ï¼Winnow ãŒçŠ¯ã™èª¤ã‚Šã®å›æ•°ã¯ï¼Œç„¡é–¢ä¿‚ãªå±æ€§ã®æ•°ã«å¯¾ã—ã¦å¯¾æ•°çš„ã«ã—ã‹å¢—åŠ ã—ãªã„ãŸã‚ï¼Œã“ã®ã‚ˆã†ãªã‚·ãƒŠãƒªã‚ªã«ãŠã„ã¦ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã®ã‚ˆã†ãªä»¥å‰ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚ˆã‚Šã‚‚è‘—ã—ãåŠ¹ç‡çš„ã§ã‚ã‚‹ï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®æ çµ„ã¿ã®ä¸­ã§ï¼Œã‚¹ãƒ‘ãƒ¼ã‚¹ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¦‚å¿µã‚’åŠ¹ç‡çš„ã«æ‰±ã†ãŸã‚ã®åŸºç¤çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¢ºç«‹ã—ãŸï¼
### 8. [Herbster and Warmuth (Machine Learning, 1998), Tracking the Best Expert](https://link.springer.com/article/10.1023/A:1007424614876)
  - This paper extends the online prediction framework to non-stationary environments, introducing an algorithm that tracks the best-performing expert whose identity may change over time, thus moving beyond simpler models that assume a single, static best expert.
  - ã“ã®è«–æ–‡ã¯ï¼Œæ™‚é–“ã®çµŒéã¨ã¨ã‚‚ã«æ€§èƒ½ãŒæœ€ã‚‚è‰¯ã„å°‚é–€å®¶ãŒå¤‰åŒ–ã—ã†ã‚‹éå®šå¸¸ç’°å¢ƒã¸ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®æ çµ„ã¿ã‚’æ‹¡å¼µã—ãŸï¼å˜ä¸€ã®é™çš„ãªæœ€è‰¯ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’ä»®å®šã™ã‚‹å˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è„±å´ã—ï¼Œå¤‰åŒ–ã™ã‚‹æœ€è‰¯ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’è¿½è·¡ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å°å…¥ã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼
### 9. ğŸŒŸ[Vovk (JCSS, 1998), A Game of Prediction with Expert Advice](https://www.sciencedirect.com/science/article/pii/S0022000097915567)
  - This paper establishes a foundational game-theoretic framework for online prediction, where the problem is modeled as a game between a predictor and an adversary. It introduces the concept of "prediction with expert advice" and provides a powerful "Aggregating Algorithm" that guarantees the predictor's loss to be close to the loss of the best expert in hindsight, thereby offering a robust theoretical underpinning for the field.
  - ã“ã®è«–æ–‡ã¯ï¼Œäºˆæ¸¬å•é¡Œã‚’äºˆæ¸¬è€…ã¨æ•µå¯¾è€…ã®é–“ã®ã‚²ãƒ¼ãƒ ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®ãŸã‚ã®åŸºç¤çš„ãªã‚²ãƒ¼ãƒ ç†è«–çš„æ çµ„ã¿ã‚’ç¢ºç«‹ã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼ã€Œã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®åŠ©è¨€ã‚’ä¼´ã†äºˆæ¸¬ã€ã¨ã„ã†æ¦‚å¿µã‚’å°å…¥ã—ï¼Œäºˆæ¸¬è€…ã®æå¤±ãŒäº‹å¾Œçš„ã«è¦‹ã¦æœ€è‰¯ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æå¤±ã«è¿‘ããªã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹å¼·åŠ›ãªã€Œé›†ç´„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã‚’æä¾›ã—ï¼Œã“ã®åˆ†é‡ã«å¼·å›ºãªç†è«–çš„åŸºç¤ã‚’ä¸ãˆãŸï¼Œ
### 10. [Kivinen and Warmuth (EuroCOLT1999), Averaging Expert Predictions](https://link.springer.com/chapter/10.1007/3-540-49097-3_13)
  - This paper is a fundamental research in online learning algorithms that generate their own predictions by averaging expert predictions, specifically proposing a method where the algorithm's prediction is a weighted average of the experts' predictions, with weights that decrease exponentially according to the loss incurred by each expert, and demonstrating that the algorithm's additional loss is bounded by the logarithm of the number of experts.
  - ã“ã®è«–æ–‡ã¯ï¼Œå°‚é–€å®¶ã®äºˆæ¸¬ã‚’å¹³å‡ã™ã‚‹ã“ã¨ã§è‡ªå·±ã®äºˆæ¸¬ã‚’ç”Ÿæˆã™ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢ã™ã‚‹åŸºç¤ç ”ç©¶ã§ã‚ã‚‹ï¼å…·ä½“çš„ã«ã¯ï¼Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®äºˆæ¸¬ãŒå°‚é–€å®¶ã®äºˆæ¸¬ã®é‡ã¿ä»˜ãå¹³å‡ã§ã‚ã‚Šï¼Œãã®é‡ã¿ã¯å„å°‚é–€å®¶ãŒè¢«ã£ãŸæå¤±ã«å¿œã˜ã¦æŒ‡æ•°é–¢æ•°çš„ã«æ¸›å°‘ã—ï¼Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¿½åŠ æå¤±ãŒå°‚é–€å®¶æ•°ã®å¯¾æ•°ã«ã‚ˆã£ã¦æŸç¸›ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼
### 11. [Kivinen and Warmuth (Machine Learning, 2001), Relative Loss Bounds for Multidimensional Regression Problems](https://link.springer.com/article/10.1023/A:1017938623079)
  - This paper extends the theoretical analysis of online learning algorithms, previously limited to one-dimensional output, to multidimensional output, and specifically derived relative loss bounds for multidimensional generalized linear regression problems. This provided a more practical evaluation criterion for multidimensional problems by eliminating probabilistic assumptions about the data and comparing the algorithm's performance to the best offline predictor.
  - ã“ã®è«–æ–‡ã¯ï¼Œãã‚Œã¾ã§ä¸€æ¬¡å…ƒå‡ºåŠ›ã«é™å®šã•ã‚Œã¦ã„ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è«–çš„è§£æã‚’å¤šæ¬¡å…ƒå‡ºåŠ›ã¸ã¨æ‹¡å¼µã—ï¼Œç‰¹ã«å¤šæ¬¡å…ƒã®ä¸€èˆ¬åŒ–ç·šå½¢å›å¸°å•é¡Œã«å¯¾ã—ã¦ç›¸å¯¾çš„ãªæå¤±é™ç•Œã‚’å°å‡ºã—ãŸï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡çš„ãªä»®å®šã‚’æ’é™¤ã—ï¼Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½ã‚’æœ€è‰¯ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³äºˆæ¸¬å™¨ã¨æ¯”è¼ƒã™ã‚‹ã¨ã„ã†ï¼Œã‚ˆã‚Šå®Ÿè·µçš„ãªè©•ä¾¡åŸºæº–ã‚’å¤šæ¬¡å…ƒå•é¡Œã«æä¾›ã—ãŸï¼
### 12. ğŸŒŸ[Zinkevich (ICML2003), Online Convex Programming and Generalized Infinitesimal Gradient Ascent](https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf)
  - This paper introduces the "Online Convex Programming" framework for uniformly handling many online prediction problems and showed that a simple gradient-descent-based algorithm can achieve a low regret of $O(\sqrt{T})$ compared to the best single decision in hindsight.
  - ã“ã®è«–æ–‡ã¯ï¼Œå¤šãã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬å•é¡Œã‚’çµ±ä¸€çš„ã«æ‰±ã†ãŸã‚ã®ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸è¨ˆç”»æ³•ã€ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ï¼Œå˜ç´”ãªå‹¾é…é™ä¸‹æ³•ã«åŸºã¥ãã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ï¼Œå¾ŒçŸ¥æµã§æœ€é©ã§ã‚ã£ãŸå˜ä¸€ã®æ±ºå®šã¨æ¯”è¼ƒã—ã¦ $O(\sqrt{T})$ ã¨ã„ã†ä½ã„ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸï¼
### 13. ğŸŒŸ[Kalai and Vempara (JCSS, 2005), Efficient algorithms for online decision problems](https://www.sciencedirect.com/science/article/pii/S0022000004001394) 
  - This paper introduces the "Follow the Perturbed Leader" (FPL) algorithm, a simple and computationally efficient method for online decision problems. Its key contribution is demonstrating that by adding random noise to the simple strategy of following the leader, it's possible to achieve performance (regret bounds) competitive with much more complex algorithms, especially in situations with a vast number of experts or actions. This opened the door for applying theoretically guaranteed online learning to many problems where it was previously computationally impractical.
  - ã“ã®è«–æ–‡ã¯ï¼Œã€ŒFollow the Perturbed Leaderã€ï¼ˆFPLï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã„ã†ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ±ºå®šå•é¡Œã«å¯¾ã™ã‚‹å˜ç´”ã‹ã¤è¨ˆç®—åŠ¹ç‡ã®è‰¯ã„æ‰‹æ³•ã‚’å°å…¥ã—ãŸï¼ãã®ä¸»è¦ãªè²¢çŒ®ã¯ï¼Œã“ã‚Œã¾ã§ã®æœ€å–„æ‰‹ã‚’é¸æŠã™ã‚‹ã¨ã„ã†å˜ç´”ãªæˆ¦ç•¥ã«ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã ã‘ã§ï¼Œç‰¹ã«ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚„è¡Œå‹•ã®é¸æŠè‚¢ãŒè†¨å¤§ãªçŠ¶æ³ã«ãŠã„ã¦ï¼Œã¯ã‚‹ã‹ã«è¤‡é›‘ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ï¼ˆãƒªã‚°ãƒ¬ãƒƒãƒˆé™ç•Œï¼‰ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸç‚¹ã«ã‚ã‚‹ï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œè¨ˆç®—ãŒéç¾å®Ÿçš„ã§ã‚ã£ãŸå¤šãã®å•é¡Œã«å¯¾ã—ã¦ï¼Œç†è«–çš„ã«æ€§èƒ½ãŒä¿è¨¼ã•ã‚ŒãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’é©ç”¨ã™ã‚‹é“ã‚’æ‹“ã„ãŸï¼
### 14. [Crammer _et al._ (JMLR, 2006), Online Passive-Aggressive Algorithms](https://jmlr.org/papers/v7/crammer06a.html)
  - This paper proposes a new framework in online learning called "Passive-Aggressive" (PA) algorithms. Its core lies in performing updates based on the margin (prediction confidence): if the prediction is correct and has a sufficient margin, it remains "passive" and does nothing, but if the margin is insufficient, it acts "aggressively" by making the minimal update required to satisfy the margin constraint. This simple and clear approach provided a powerful online learning method that is computationally efficient and has theoretical performance guarantees (regret bounds), especially for large-scale datasets and data streams.
  - ã“ã®è«–æ–‡ã¯ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ãŠã‘ã‚‹ã€Œãƒ‘ãƒƒã‚·ãƒ–ãƒ»ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ã€ï¼ˆPAï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã„ã†æ–°ãŸãªæ çµ„ã¿ã‚’æå”±ã—ãŸï¼ãã®æ ¸å¿ƒã¯ï¼Œãƒãƒ¼ã‚¸ãƒ³ï¼ˆäºˆæ¸¬ã®ä¿¡é ¼åº¦ï¼‰ã«åŸºã¥ã„ã¦æ›´æ–°ã‚’è¡Œã†ç‚¹ã«ã‚ã‚Šï¼Œäºˆæ¸¬ãŒæ­£ã—ãï¼Œã‹ã¤ååˆ†ãªãƒãƒ¼ã‚¸ãƒ³ãŒã‚ã‚‹å ´åˆã¯ã€Œãƒ‘ãƒƒã‚·ãƒ–ã€ã«ä½•ã‚‚ã—ãªã„ãŒï¼Œãƒãƒ¼ã‚¸ãƒ³ãŒä¸ååˆ†ãªå ´åˆã¯ã€Œã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ã€ã«ï¼Œãƒãƒ¼ã‚¸ãƒ³åˆ¶ç´„ã‚’æº€ãŸã™æœ€å°é™ã®æ›´æ–°ã‚’è¡Œã†ã¨ã„ã†ã‚‚ã®ã§ã‚ã‚‹ï¼ã“ã®å˜ç´”æ˜å¿«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šï¼Œç‰¹ã«å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ï¼Œè¨ˆç®—åŠ¹ç‡ãŒé«˜ãï¼Œã‹ã¤ç†è«–çš„ãªæ€§èƒ½ä¿è¨¼(ãƒªã‚°ãƒ¬ãƒƒãƒˆé™ç•Œ)ã‚’æŒã¤å¼·åŠ›ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã‚’æä¾›ã—ãŸï¼
### 15. [Hazan _et al._ (Machine Learning, 2007), Logarithmic Regret Algorithms for Online Convex Optimization](https://link.springer.com/article/10.1007/s10994-007-5016-8)
  - This paper significantly improves upon the previous general regret bound of $O(\sqrt{T})$ in online convex optimization. Its core contribution lies in being the first to propose an algorithm that achieves logarithmic regret, $O(\log{T})$, under the condition that the loss functions possess the property of "strong convexity." This demonstrates that for a specific (yet important) class of problems, online learning can converge to the optimal solution much more rapidly, thus opening a new frontier in regret analysis.
  - ã“ã®è«–æ–‡ã¯ï¼Œãã‚Œã¾ã§ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªãƒªã‚°ãƒ¬ãƒƒãƒˆé™ç•Œã§ã‚ã£ãŸ $O(\sqrt{T})$ ã‚’å¤§å¹…ã«æ”¹å–„ã™ã‚‹ï¼ãã®æ ¸å¿ƒçš„ãªè²¢çŒ®ã¯ï¼Œæå¤±é–¢æ•°ãŒã€Œå¼·å‡¸æ€§ã€ã¨ã„ã†æ€§è³ªã‚’æŒã¤å ´åˆã«é™å®šã™ã‚Œã°ï¼Œ$O(\log{T})$ ã¨ã„ã†å¯¾æ•°çš„ãªãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã§ãã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’åˆã‚ã¦ææ¡ˆã—ãŸç‚¹ã«ã‚ã‚‹ï¼Œã“ã‚Œã«ã‚ˆã‚Šï¼Œç‰¹å®šã®ï¼ˆã—ã‹ã—é‡è¦ãªï¼‰å•é¡Œã‚¯ãƒ©ã‚¹ã«ãŠã„ã¦ã¯ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãŒã¯ã‚‹ã‹ã«é€Ÿãæœ€é©è§£ã«åæŸã™ã‚‹ã“ã¨ã‚’ç¤ºã—ï¼Œãƒªã‚°ãƒ¬ãƒƒãƒˆè§£æã®æ–°ãŸãªåœ°å¹³ã‚’åˆ‡ã‚Šæ‹“ã„ãŸï¼
### 16. [Cesa-Bianchi _et al._ (Machine Learning, 2007), Improved second-order bounds for prediction with expert advice](https://arxiv.org/abs/math/0602629)
  - This paper re-evaluates the performance of online prediction with expert advice using a more precise measure called "second-order bounds." Whereas previous first-order bounds related regret only to the cumulative loss of the best expert, this research derived tighter regret bounds by taking into account the "variance" of the experts' losses. This demonstrated that in situations where expert losses are stable, the algorithm can perform significantly better than previously thought, adding a new layer of depth to the performance analysis of online learning algorithms.
  - ã“ã®è«–æ–‡ã¯ï¼Œã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‹ã‚‰ã®åŠ©è¨€ã«ã‚ˆã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®æ€§èƒ½è©•ä¾¡ã‚’ï¼Œã‚ˆã‚Šç²¾å¯†ãªã€ŒäºŒæ¬¡ã®é™ç•Œ (second-order bounds) ã€ã¨ã„ã†å°ºåº¦ã§æ‰ãˆç›´ã—ãŸï¼å¾“æ¥ã®ä¸€æ¬¡ã®é™ç•ŒãŒãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’æœ€è‰¯ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç´¯ç©æå¤±ã®ã¿ã«é–¢é€£ä»˜ã‘ã¦ã„ãŸã®ã«å¯¾ã—ï¼Œã“ã®ç ”ç©¶ã§ã¯ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŸã¡ã®æå¤±ã®ã€Œåˆ†æ•£ã€ã‚’è€ƒæ…®ã«å…¥ã‚Œã‚‹ã“ã¨ã§ï¼Œã‚ˆã‚Šã‚¿ã‚¤ãƒˆãªãƒªã‚°ãƒ¬ãƒƒãƒˆé™ç•Œã‚’å°å‡ºã—ãŸï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŸã¡ã®æå¤±ãŒå®‰å®šã—ã¦ã„ã‚‹çŠ¶æ³ã§ã¯ï¼Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¾“æ¥è€ƒãˆã‚‰ã‚Œã¦ã„ãŸã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½è§£æã«æ–°ãŸãªæ·±ã¿ã‚’ä¸ãˆãŸã‚‚ã®ã§ã‚ã‚‹ï¼
### 17. [Crammer _et al._ (EMNLP2009), Multi-Class Confidence Weighted Algorithms](https://aclanthology.org/D09-1052/)
  - This paper extends "Confidence-Weighted" (CW) learning, a sophisticated evolution of Passive-Aggressive algorithms, to the multi-class classification setting. Its groundbreaking aspect is that instead of updating a single parameter vector, it models parameter uncertainty with a multivariate Gaussian distribution and adjusts the magnitude of updates according to its "confidence" (the inverse of the variance). By updating the weights of low-confidence (infrequently observed) features more aggressively and high-confidence weights more conservatively, it enables more informed and efficient learning, especially for high-dimensional, sparse data common in fields like Natural Language Processing, significantly influencing subsequent online learning research.
  - ã“ã®è«–æ–‡ã¯ï¼Œãã‚Œã¾ã§ã®ãƒ‘ãƒƒã‚·ãƒ–ãƒ»ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãªã©ã‚’ç™ºå±•ã•ã›ï¼Œã€Œä¿¡é ¼åº¦åŠ é‡ã€ï¼ˆConfidence-Weighted, CWï¼‰å­¦ç¿’ã¨ã„ã†ï¼Œã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã‚’å¤šã‚¯ãƒ©ã‚¹åˆ†é¡å•é¡Œã¸ã¨æ‹¡å¼µã—ãŸï¼å˜ä¸€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ›´æ–°ã™ã‚‹ã®ã§ã¯ãªãï¼Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ã‚’å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ï¼Œãã®ã€Œä¿¡é ¼åº¦ã€(åˆ†æ•£ã®é€†æ•°)ã«å¿œã˜ã¦æ›´æ–°ã®åº¦åˆã„ã‚’èª¿æ•´ã™ã‚‹ã¨ã„ã†ç‚¹ãŒç”»æœŸçš„ã§ã‚ã‚‹ï¼Œä¿¡é ¼åº¦ãŒä½ã„(ã‚ã¾ã‚Šè¦³æ¸¬ã•ã‚Œã¦ã„ãªã„)ç‰¹å¾´ã®é‡ã¿ã¯å¤§ããï¼Œä¿¡é ¼åº¦ãŒé«˜ã„é‡ã¿ã¯å°ã•ãæ›´æ–°ã™ã‚‹ã“ã¨ã§ï¼Œç‰¹ã«è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã§è¦‹ã‚‰ã‚Œã‚‹é«˜æ¬¡å…ƒã§ç–ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ï¼Œã‚ˆã‚Šæƒ…å ±é‡ã«åŸºã¥ã„ãŸåŠ¹ç‡çš„ãªå­¦ç¿’ã‚’å¯èƒ½ã«ã—ï¼Œãã®å¾Œã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç ”ç©¶ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆãŸï¼
### 18. ğŸŒŸ[Crammer _et al._ (NeurIPS2009), Adaptive Regularization of Weight Vectors](https://papers.nips.cc/paper_files/paper/2009/hash/8ebda540cbcc4d7336496819a46a1b68-Abstract.html)
  - This paper introduces the "Adaptive Regularization of Weight Vectors" (AROW) algorithm, which overcomes a key limitation of preceding methods like Confidence-Weighted (CW) learning: vulnerability to noisy data. Its primary role is to significantly improve robustness by incorporating a soft-margin objective while inheriting the concept of "confidence" from CW (representing parameter uncertainty with a covariance matrix). This established a more practical algorithm that enjoys the benefits of second-order adaptive updates while remaining stable against mislabeled examples.
  - ã“ã®è«–æ–‡ã¯ï¼Œå…ˆè¡Œç ”ç©¶ã§ã‚ã‚‹ä¿¡é ¼åº¦åŠ é‡ (CW) å­¦ç¿’ã®èª²é¡Œï¼Œã™ãªã‚ã¡ãƒã‚¤ã‚ºã®å¤šã„ãƒ‡ãƒ¼ã‚¿ã¸ã®è„†å¼±æ€§ã‚’å…‹æœã™ã‚‹ã€Œé‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®é©å¿œçš„æ­£å‰‡åŒ–ã€(AROW) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ãŸï¼ãã®ä¸»è¦ãªå½¹å‰²ã¯ï¼ŒCW ãŒæŒã¤ã€Œä¿¡é ¼åº¦ã€ã®æ¦‚å¿µï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸ç¢ºå®Ÿæ€§ã‚’å…±åˆ†æ•£è¡Œåˆ—ã§è¡¨ç¾ã™ã‚‹ç‚¹ï¼‰ã‚’å¼•ãç¶™ãã¤ã¤ï¼Œã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³ç›®çš„ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ï¼Œé ‘å¥æ€§ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ãŸç‚¹ã«ã‚ã‚‹ï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œä¿¡é ¼åº¦ã«åŸºã¥ã„ãŸé©å¿œçš„ãªæ›´æ–°ã¨ã„ã†äºŒæ¬¡ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã®åˆ©ç‚¹ã‚’äº«å—ã—ãªãŒã‚‰ï¼Œèª¤ã£ãŸãƒ©ãƒ™ãƒ«ã‚’æŒã¤ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦ã‚‚å®‰å®šã—ãŸå­¦ç¿’ã‚’å¯èƒ½ã«ã—ï¼Œã‚ˆã‚Šå®Ÿç”¨çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç¢ºç«‹ã—ãŸã‚‚ã®ã§ã‚ã‚‹ï¼
### 19. ğŸŒŸ[Duchi _et al._ (JMLR, 2010), Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.org/papers/v12/duchi11a.html)
  - This paper shatters the limitations of previous online learning methods using fixed learning rates by proposing "Adaptive Subgradient Methods," most notably its prime example, "AdaGrad," which adaptively adjusts the learning rate for each individual parameter (feature). Its core innovation lies in automatically adjusting the learning rate based on the magnitude of past gradients, applying smaller learning rates to frequently occurring features and larger rates to infrequent ones. This mechanism dramatically improved learning efficiency and performance, especially for the high-dimensional, sparse data common in fields like Natural Language Processing, and laid the foundation for many subsequent adaptive learning rate algorithms (e.g., RMSProp, Adam).
  - ã“ã®è«–æ–‡ã¯ï¼Œãã‚Œã¾ã§ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã§ä¸€èˆ¬çš„ã§ã‚ã£ãŸå›ºå®šã®å­¦ç¿’ç‡ã‚’ç”¨ã„ã‚‹æ‰‹æ³•ã®é™ç•Œã‚’æ‰“ã¡ç ´ã‚Šï¼Œå€‹ã€…ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´ï¼‰ã”ã¨ã«å­¦ç¿’ç‡ã‚’é©å¿œçš„ã«èª¿æ•´ã™ã‚‹ã€Œé©å¿œçš„åŠ£å‹¾é…æ³•ã€(Adaptive Subgradient Methods)ï¼Œç‰¹ã«ãã®ä»£è¡¨æ ¼ã§ã‚ã‚‹ã€ŒAdaGradã€ã‚’ææ¡ˆã—ãŸï¼ãã®æ ¸å¿ƒã¯ï¼Œéå»ã®å‹¾é…ã®å¤§ãã•ã«å¿œã˜ã¦å­¦ç¿’ç‡ã‚’è‡ªå‹•ã§èª¿æ•´ã™ã‚‹ç‚¹ã«ã‚ã‚Šï¼Œé »ç¹ã«ç¾ã‚Œã‚‹ç‰¹å¾´ã«ã¯å°ã•ãªå­¦ç¿’ç‡ã‚’ï¼Œç¨€ã«ã—ã‹ç¾ã‚Œãªã„ç‰¹å¾´ã«ã¯å¤§ããªå­¦ç¿’ç‡ã‚’é©ç”¨ã™ã‚‹ï¼Œã“ã®ä»•çµ„ã¿ã«ã‚ˆã‚Šï¼Œç‰¹ã«è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã§é »å‡ºã™ã‚‹é«˜æ¬¡å…ƒã§ç–ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ï¼Œå­¦ç¿’ã®åŠ¹ç‡ã¨æ€§èƒ½ã‚’åŠ‡çš„ã«å‘ä¸Šã•ã›ï¼Œãã®å¾Œã®å¤šãã®é©å¿œçš„å­¦ç¿’ç‡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (ä¾‹:RMSProp, Adam) ã®ç¤ã‚’ç¯‰ã„ãŸã‚‚ã®ã§ã‚ã‚‹ï¼
### 20. [McDonald _et al._ (NAACL HLT2010), Distributed Training Strategies for the Structured Perceptron](https://aclanthology.org/N10-1069.pdf)
  - This paper is the first to systematically propose and compare "distributed training strategies" for the structured perceptron, for which single-machine training was previously the mainstream, to efficiently train it on large-scale datasets. Its key role lies in introducing various distributed strategies, from simple mini-batch learning to the more sophisticated iterative parameter mixing, and experimentally clarifying their impact on training speed and final prediction performance. By doing so, it showed a concrete path for adapting online learning algorithms to the era of big data and contributed significantly to the subsequent development of distributed learning research.
  - ã“ã®è«–æ–‡ã¯ï¼Œãã‚Œã¾ã§å˜ä¸€ã®ãƒã‚·ãƒ³ã§ã®å­¦ç¿’ãŒä¸»æµã§ã‚ã£ãŸæ§‹é€ åŒ–ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ã‚’ï¼Œå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦åŠ¹ç‡çš„ã«å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ã€Œåˆ†æ•£å­¦ç¿’æˆ¦ç•¥ã€ã‚’åˆã‚ã¦ä½“ç³»çš„ã«ææ¡ˆãƒ»æ¯”è¼ƒã—ãŸï¼ãã®é‡è¦ãªå½¹å‰²ã¯ï¼Œå˜ç´”ãªãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã‹ã‚‰ï¼Œã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸåå¾©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ··åˆ (iterative parameter mixing) ã¾ã§ï¼Œæ§˜ã€…ãªåˆ†æ•£æˆ¦ç•¥ã‚’å°å…¥ã—ï¼Œãã‚Œã‚‰ãŒå­¦ç¿’é€Ÿåº¦ã¨æœ€çµ‚çš„ãªäºˆæ¸¬æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®Ÿé¨“çš„ã«æ˜ã‚‰ã‹ã«ã—ãŸç‚¹ã«ã‚ã‚‹ï¼ã“ã‚Œã«ã‚ˆã‚Šï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã®æ™‚ä»£ã«é©å¿œã•ã›ã‚‹ãŸã‚ã®å…·ä½“çš„ãªé“ç­‹ã‚’ç¤ºã—ï¼Œå¾Œã®åˆ†æ•£å­¦ç¿’ç ”ç©¶ã®ç™ºå±•ã«å¤§ããè²¢çŒ®ã—ãŸï¼
### 21. [Chu _et al._ (KDD2011), Unbiased online active learning in data streams](https://dl.acm.org/doi/10.1145/2020408.2020444)
  - Awesome paper that addresses unbiased online active learning in data streams, providing theoretical guarantees for selective sampling strategies.
  - ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¯¾ã™ã‚‹ä¸åã‚ªãƒ³ãƒ©ã‚¤ãƒ³èƒ½å‹•å­¦ç¿’ã«å–ã‚Šçµ„ã¿ï¼Œé¸æŠçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã®ç†è«–ä¿è¨¼ã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 22. [Shalev-Shwartz _et al._ (Mathematical Programming, 2017), Pegasos: primal estimated sub-gradient solver for SVM](https://link.springer.com/article/10.1007/s10107-010-0420-4)
  - Awesome paper that introduces Pegasos algorithm for SVM training with convergence guarantees.
  - SVMï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼‰ã®å­¦ç¿’ã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚ã‚‹Pegasosã‚’ææ¡ˆã—ï¼ŒåæŸä¿è¨¼ã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 23. ğŸŒŸ[Cesa-Bianchi and Lugosi (JCSS, 2012), Combinatorial bandits](https://www.sciencedirect.com/science/article/pii/S0022000012000219)
  - Awesome paper that introduces combinatorial bandits framework, extending multi-armed bandits to combinatorial action spaces with efficient algorithms.
  - çµ„åˆã›è«–çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã®æ çµ„ã¿ã‚’å°å…¥ã—ï¼Œå¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚’çµ„åˆã›è¡Œå‹•ç©ºé–“ã«æ‹¡å¼µã—ã¦åŠ¹ç‡çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›ã—ãŸè«–æ–‡ï¼
### 24. ğŸŒŸ[Suehiro _et al._ (ALT2012), Online Prediction under Submodular Constraints](https://api.lib.kyushu-u.ac.jp/opac_download_md/1932327/alt12.pdf)
  - Awesome paper that studies online prediction under submodular constraints, providing regret bounds for constrained online learning problems.
  - åŠ£ãƒ¢ã‚¸ãƒ¥ãƒ©åˆ¶ç´„ä¸‹ã§ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«å–ã‚Šçµ„ã¿ï¼Œåˆ¶ç´„ä»˜ãã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’å•é¡Œã®ãƒªã‚°ãƒ¬ãƒƒãƒˆå¢ƒç•Œã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 25. [Wang _et al._ (ICML2012), Exact Soft Confidence-Weighted Learning](https://arxiv.org/abs/1206.4612)
  - Awesome paper that develops exact soft confidence-weighted learning, providing precise probabilistic updates for online classification with theoretical guarantees.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†é¡ã«ãŠã‘ã‚‹ï¼Œç†è«–ä¿è¨¼ä»˜ãã®ã‚ˆã‚Šå„ªã‚ŒãŸç¢ºç‡çš„æ›´æ–°ã¨ã—ã¦ã€ŒExact Soft Confidence-Weighted Learningã€ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 26. ğŸŒŸ[Bubeck and Slivkins (COLT2012), The best of both worlds: stochastic and adversarial bandits](http://sbubeck.com/COLT12_BS.pdf)
  - Awesome paper that achieves the best of both worlds in bandits, providing algorithms that perform well in both stochastic and adversarial settings simultaneously.
  - ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã§ç¢ºç‡çš„ãƒ»æ•µå¯¾çš„ä¸¡è¨­å®šã§åŒæ™‚ã«æœ€è‰¯æ€§èƒ½ã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 27. [Neu and BartÃ³k (ALT2013), An efficient algorithm for learning with semi-bandit feedback](https://arxiv.org/abs/1305.2732)
  - Awesome paper that provides efficient algorithms for semi-bandit feedback, extending bandit learning to partial information settings with multiple simultaneous actions.
  - ã‚»ãƒŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’ã™ã‚‹åŠ¹ç‡çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›ã—ï¼Œãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå­¦ç¿’ã‚’è¤‡æ•°åŒæ™‚è¡Œå‹•ã‚’ä¼´ã†éƒ¨åˆ†æƒ…å ±è¨­å®šã«æ‹¡å¼µã—ãŸè«–æ–‡ï¼
### 28. [Gaillard _et al._ (COLT2014), A Second-order Bound with Excess Losses](https://arxiv.org/abs/1402.2044)
  - Awesome paper that derives second-order regret bounds with excess losses, providing tighter analysis for online learning algorithms using variance information.
  - éå‰°æå¤±ã‚’ç”¨ã„ãŸ2æ¬¡ãƒªã‚°ãƒ¬ãƒƒãƒˆå¢ƒç•Œã‚’å°å‡ºã—ï¼Œåˆ†æ•£æƒ…å ±ã‚’æ´»ç”¨ã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚Šå³å¯†ãªè§£æã‚’ä¸ãˆãŸè«–æ–‡ï¼
### 29. ğŸŒŸ[Kingma and Ba (ICLR2015), Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
  - Awesome paper that introduces Adam optimizer combining advantages of AdaGrad and RMSprop, becoming the most widely used adaptive optimization method.
  - AdaGradã¨RMSpropã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ãŸAdamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’å°å…¥ã—ï¼Œæœ€ã‚‚åºƒãä½¿ç”¨ã•ã‚Œã‚‹é©å¿œçš„æœ€é©åŒ–æ‰‹æ³•ã¨ãªã£ãŸè«–æ–‡ï¼
### 30. [Luo and Schapire (COLT2015), Achieving All with No Parameters: AdaNormalHedge](https://proceedings.mlr.press/v40/Luo15.pdf)
  - Awesome paper that proposes AdaNormalHedge, a truly parameter-free algorithm for expert advice that simultaneously achieves multiple objectives without prior information including adaptive regret and unknown competitor performance. 
  - ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã«ãŠã„ã¦ï¼Œäº‹å‰æƒ…å ±ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æ•°ãªã©ï¼‰ã‚’å¿…è¦ã¨ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ãƒªãƒ¼ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ŒAdaNormalHedgeã€ã‚’ææ¡ˆï¼
### 31. [Hazan _et al._ (ICML2017), Efficient Regret Minimization in Non-Convex Games](https://proceedings.mlr.press/v70/hazan17a.html)
  - Awesome paper that studies online algorithms for non-convex loss functions, defining a new measure called "local regret" based on projected gradient magnitudes from time-smoothed losses and proposing algorithms to efficiently minimize it.
  - éå‡¸ãªæå¤±é–¢æ•°ã‚’å¯¾è±¡ã¨ã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç ”ç©¶ï¼æ™‚é–“å¹³æ»‘åŒ–ã—ãŸæå¤±ï¼ˆéå»kå›ã®æå¤±ã‚’å¹³å‡ã—ãŸã‚‚ã®ï¼‰ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹å°„å½±å‹¾é…ã®å¤§ãã•ã‚’åŸºã«ã—ãŸæ–°ã—ã„å°ºåº¦ã€Œå±€æ‰€ãƒªã‚°ãƒ¬ãƒƒãƒˆã€ã‚’å®šç¾©ã—ï¼Œãã‚Œã‚’åŠ¹ç‡çš„ã«æœ€å°åŒ–ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆï¼
  ### 32. ğŸŒŸ[Zheng and Kwok (ICML2017), Follow the Moving Leader in Deep Learning](https://proceedings.mlr.press/v70/zheng17a.html)
  - Awesome paper that proposes Follow the Moving Leader (FTML), a variant of FTRL for deep learning optimization that adapts quickly to changes by weighting recent samples more heavily. 
  - æ·±å±¤å­¦ç¿’ã®æœ€é©åŒ–ã«ãŠã„ã¦ï¼Œæœ€è¿‘ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚ˆã‚Šé‡ãé‡ã¿ä»˜ã‘ã™ã‚‹ã“ã¨ã§å¤‰åŒ–ã«ç´ æ—©ãé©å¿œã™ã‚‹Follow the Moving Leader (FTML) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 33. ğŸŒŸ[Zhang _et al._ (NeurIPS2018), Adaptive Online Learning in Dynamic Environments](https://arxiv.org/abs/1810.10815)
  - Awesome paper that first establishes theoretical lower bounds for dynamic regret against arbitrary comparison sequences and proposes "Ader" which adaptively combines multiple OGD experts with different step sizes using a meta-algorithm.
  - ä»»æ„ã®æ¯”è¼ƒå¯¾è±¡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã®ç†è«–çš„ãªä¸‹é™ã‚’åˆã‚ã¦æç¤ºã—ãŸç ”ç©¶ï¼ç†è«–çš„ãªä¸‹é™ã¨ä¸€èˆ¬çš„ãªOGDã¨ã®å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã«ä¹–é›¢ãŒã‚ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ï¼Œãã®è§£æ±ºç­–ã¨ã—ã¦ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’æŒã¤è¤‡æ•°ã®OGDï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼‰ã‚’ãƒ¡ã‚¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§é©å¿œçš„ã«çµ±åˆã™ã‚‹æ‰‹æ³•ã€ŒAderã€ã‚’ææ¡ˆï¼
### 34. [Finn _et al._ (ICLR2019), Online Meta-Learning](https://arxiv.org/abs/1902.08438)
  - Awesome paper that introduces online meta-learning which merges ideas from meta-learning and online learning, proposing the follow the meta leader algorithm extending MAML with O(log T) regret guarantee.
  - ãƒ¡ã‚¿å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’èåˆã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¡ã‚¿å­¦ç¿’ã‚’å°å…¥ã—ï¼ŒMAMLã‚’æ‹¡å¼µã—ãŸfollow the meta leaderã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’O(log T)ã®regretä¿è¨¼ã¨ã¨ã‚‚ã«ææ¡ˆã—ãŸè«–æ–‡ï¼
### 35. [Zhao _et al._ (NeurIPS2020), Dynamic Regret of Convex and Smooth Functions](https://arxiv.org/abs/2007.03479)
  - Awesome paper that enhances dynamic regret bounds by exploiting smoothness conditions, replacing the dependence on T with problem-dependent quantities like gradient variation and comparator loss, making bounds adaptive to problem difficulty.
  - å‡¸ã‹ã¤æ»‘ã‚‰ã‹ãªé–¢æ•°ã«å¯¾ã™ã‚‹å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’ç ”ç©¶ã—ï¼Œæ»‘ã‚‰ã‹ã•æ¡ä»¶ã‚’æ´»ç”¨ã—ã¦Tã¸ã®ä¾å­˜ã‚’å‹¾é…å¤‰å‹•ã‚„æ¯”è¼ƒå¯¾è±¡ã®æå¤±ãªã©ã®å•é¡Œä¾å­˜é‡ã«ç½®ãæ›ãˆï¼Œå•é¡Œã®é›£æ˜“åº¦ã«é©å¿œçš„ãªå¢ƒç•Œã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 36. [Ito (NeurIPS2021), On Optimal Robustness to Adversarial Corruption in Online Decision Problems](https://arxiv.org/abs/2109.10963)
  - Awesome paper that studies optimal robustness to adversarial corruption in online decision problems, analyzing how online algorithms can maintain performance guarantees when a fraction of inputs are adversarially corrupted.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ±ºå®šå•é¡Œã«ãŠã‘ã‚‹æ•µå¯¾çš„æ‘‚å‹•ã«å¯¾ã™ã‚‹æœ€é©ãªé ‘å¥æ€§ã‚’ç ”ç©¶ã—ï¼Œå…¥åŠ›ã®ä¸€éƒ¨ã«æ•µå¯¾çš„ãªæ‘‚å‹•ãŒåŠ ã‚ã£ãŸå ´åˆã§ã‚‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ€§èƒ½ä¿è¨¼ã‚’ç¶­æŒã™ã‚‹æ–¹æ³•ã‚’è§£æã—ãŸè«–æ–‡ï¼
### 37. ğŸŒŸ[Zimmert and Seldin (JMLR, 2021), Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits](https://arxiv.org/abs/1807.07623)
  - Awesome paper that derives Tsallis-INF algorithm using online mirror descent with Tsallis entropy regularization, achieving optimal regret in both stochastic and adversarial multi-armed bandits.
  - Tsallisã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã‚’ç”¨ã„ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒŸãƒ©ãƒ¼é™ä¸‹æ³•ã«åŸºã¥ãTsallis-INFã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å°å‡ºã—ï¼Œç¢ºç‡çš„ãƒ»æ•µå¯¾çš„å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆä¸¡æ–¹ã§æœ€é©ãªãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 38. ğŸŒŸ[Baby _et al._ (NeurIPS2023), Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms](https://neurips.cc/virtual/2023/poster/71994)
  - Awesome paper that tackles online learning under changing data distributions, developing practical algorithms that automatically adapt to distribution shifts without prior knowledge while achieving optimal theoretical guarantees.
  - ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒå¤‰åŒ–ã™ã‚‹ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç’°å¢ƒã«ãŠã„ã¦ï¼Œåˆ†å¸ƒã‚·ãƒ•ãƒˆã«äº‹å‰çŸ¥è­˜ãªã—ã§è‡ªå‹•é©å¿œã™ã‚‹å®Ÿç”¨çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é–‹ç™ºã—ï¼Œæœ€é©ãªç†è«–ä¿è¨¼ã‚’é”æˆã—ãŸè«–æ–‡ï¼
### 39. [Dai _et al._ (CVPR2025), Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret](https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal_CVPR_2025_paper.html)
  - Awesome paper that addresses label shift in online learning settings by constructing a novel convex risk estimator and enhanced online algorithm, achieving minimax optimal universal dynamic regret.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç’°å¢ƒã§ã®ãƒ©ãƒ™ãƒ«ã‚·ãƒ•ãƒˆå•é¡Œã«å¯¾ã—ã¦æ–°ã—ã„å‡¸ãƒªã‚¹ã‚¯æ¨å®šå™¨ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ§‹ç¯‰ã—ï¼Œminimaxæœ€é©ãªæ±ç”¨å‹•çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆã‚’é”æˆã—ãŸè«–æ–‡ï¼

## Awesome-Applied Researchï¼ˆå¿œç”¨ç ”ç©¶ï¼‰
### 40. ğŸŒŸ[Bashratat _et al._ (CVPR2008), Learning object motion patterns for anomaly detection and improved object detection](https://ieeexplore.ieee.org/document/4587510)
  - Awesome paper that learns object motion patterns in surveillance videos for anomaly detection and improved object detection.
  - ç›£è¦–æ˜ åƒã«ãŠã‘ã‚‹å¿œç”¨ã¨ã—ã¦ï¼Œç‰©ä½“ã®å‹•ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã—ï¼Œç•°å¸¸æ¤œçŸ¥ã¨ç‰©ä½“æ¤œå‡ºã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 41. [Arora _et al._ (Theory of Computing, 2012), The Multiplicative Weights Update Method: a Meta-Algorithm and Applications](https://theoryofcomputing.org/articles/v008a006/)
  - Awesome paper that presents multiplicative weights update as a meta-algorithm with broad applications across computer science and optimization.
  - ä¹—æ³•çš„é‡ã¿æ›´æ–°ã‚’ãƒ¡ã‚¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦æ‰ãˆï¼Œãã®å¹…åºƒã„å¿œç”¨ã‚’ç¤ºã—ãŸè«–æ–‡ï¼
### 42. [Ho _et al._(NeurIPS2013), More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server](https://fid3024.github.io/papers/2013%20-%20More%20Effective%20Distributed%20ML%20via%20a%20Stale%20Sychronous%20Parallel%20Parameter%20Server.pdf)
  - Awesome paper that introduces Stale Synchronous Parallel Parameter Server for distributed machine learning, improving efficiency through asynchronous updates.
  - åˆ†æ•£æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®ã€ŒStale Synchronous Parallel Parameter Serverã€ã‚’å°å…¥ã—ï¼ŒéåŒæœŸæ›´æ–°ã«ã‚ˆã‚ŠåŠ¹ç‡æ€§ã‚’å‘ä¸Šã•ã›ãŸè«–æ–‡ï¼
### 43. [McMahan _et al._ (KDD2013), Ad Click Prediction: a View from the Trenches](https://research.google/pubs/ad-click-prediction-a-view-from-the-trenches/)
  - Awesome paper that presents practical insights for ad click prediction from large-scale deployment, bridging theory and real-world online learning applications.
  - å¤§è¦æ¨¡ãªãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã«ãŠã‘ã‚‹åºƒå‘Šã‚¯ãƒªãƒƒã‚¯äºˆæ¸¬ã«å–ã‚Šçµ„ã¿ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ç†è«–ã¨å®Ÿä¸–ç•Œã«ãŠã‘ã‚‹å¿œç”¨ã‚’æ©‹æ¸¡ã—ã—ãŸè«–æ–‡ï¼
### 44. [Grnarova _et al._ (ICLR2018), An Online Learning Approach to Generative Adversarial Networks](https://arxiv.org/abs/1706.03269)
  - Awesome paper that applies online learning techniques to GAN training for improved stability.
  - GANã®è¨“ç·´ã«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã‚’é©ç”¨ã—ã¦å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ãŸè«–æ–‡ï¼
### 45. [Song _et al._ (Machine Learning, 2024), No Regret Sample Selection with Noisy Labels](https://arxiv.org/abs/2003.03179)
  - Awesome paper that proposes adaptive k-set selection for training DNNs with noisy labels while providing theoretical regret bounds. 
  - ãƒã‚¤ã‚¸ãƒ¼ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã§ã®DNNè¨“ç·´ã«ãŠã„ã¦ï¼Œç†è«–çš„ãªãƒªã‚°ãƒ¬ãƒƒãƒˆä¿è¨¼ã‚’æŒã¤é©å¿œçš„k-seté¸æŠæ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 46. [Song _et al._ (WACV2020), Adaptive Aggregation of Arbitrary Online Trackers with a Regret Bound](https://openaccess.thecvf.com/content_WACV_2020/papers/Song_Adaptive_Aggregation_of_Arbitrary_Online_Trackers_with_a_Regret_Bound_WACV_2020_paper.pdf)
  - Awesome paper that proposes delayed-Hedge algorithm for aggregating arbitrary online trackers with theoretical regret guarantees in adversarial environments.
  - æ•µå¯¾çš„ç’°å¢ƒã«ãŠã„ã¦ç†è«–çš„ãƒªã‚°ãƒ¬ãƒƒãƒˆä¿è¨¼ã‚’æŒã¤delayed-Hedgeã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ï¼Œä»»æ„ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’é›†ç´„ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼
### 47. [Matsuo _et al._ (ICASSP2023), Learning from Label Proportion with Online Pseudo-Label Decision by Regret Minimization](https://arxiv.org/abs/2302.08947)
  - Awesome paper that proposes online pseudo-labeling with regret minimization for Learning from Label Proportions, effectively handling large bag sizes.
  - Learning from Label Proportions (LLP) ã«ãŠã„ã¦ï¼Œãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ã«ã‚ˆã‚‹æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ±ºå®šæ‰‹æ³•ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼å¤§ããªbagã‚µã‚¤ã‚ºã§ã‚‚åŠ¹æœçš„ã«å‹•ä½œã™ã‚‹ï¼
### 48. [Å vihrovÃ¡ _et al._ (Frontiers in Digital Health, 2025), Designing digital health interventions with causal inference and multi-armed bandits: a review](https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1435917/full)
  - Awesome paper that reviews how to design digital health interventions using multi-armed bandits and causal inference for Just-In-Time Adaptive Interventions in behavioral change support systems.
  - ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢åˆ†é‡ã«ãŠã‘ã‚‹Just-In-Time Interventionã«å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨å› æœè§£æã‚’å°å…¥ã™ã‚‹æ–¹æ³•è«–ã«é–¢ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼è«–æ–‡ï¼è¢«é¨“è€…ã®å¥åº·çŠ¶æ…‹ã‚’é€æ¬¡çš„ã«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã—ï¼Œé©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§"ä»‹å…¥"ã—ã¦è¡Œå‹•å¤‰å®¹ã‚’ä¿ƒã™æ çµ„ã¿ï¼
### 49. [Kumar _et al._ (AAAI2024), Using adaptive bandit experiments to increase and investigate engagement in mental health](https://ojs.aaai.org/index.php/AAAI/article/view/30328)
  - Awesome paper that presents a software system using Thompson Sampling bandit algorithms for adaptive text-message-based mental health interventions, evaluated with 1100 users to simultaneously improve engagement and collect data for analysis.
  - ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹ã«ãŠã‘ã‚‹å€‹åˆ¥åŒ–åŒ»ç™‚ã«ãŠã„ã¦Thompson Samplingã‚’ç”¨ã„ãŸãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã—ï¼Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨1100äººã®ãƒ¦ãƒ¼ã‚¶ã§å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ã—ãŸè«–æ–‡ï¼
### 50. [GutiÃ©rrez _et al._ (MICCAI2017), A Multi-armed Bandit to Smartly Select a Training Set from Big Medical Data](https://link.springer.com/chapter/10.1007/978-3-319-66179-7_5)
  - Awesome paper that formulates efficient training set selection from large-scale medical imaging data as a multi-armed bandit problem, using clustering-based exploration-exploitation for age prediction from brain images.
  - å¤§è¦æ¨¡ãªåŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é©åˆ‡ã‹ã¤åŠ¹ç‡çš„ã«é¸æŠã™ã‚‹å•é¡Œã‚’å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨ã—ã¦å®šå¼åŒ–ã—ï¼Œè„³ç”»åƒã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ç‰¹å¾´é‡ã‹ã‚‰å¹´é½¢ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œã«å–ã‚Šçµ„ã‚“ã è«–æ–‡ï¼äº‹å‰ã«è¦³æ¸¬ã§ãã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹ã¨ã„ã†å•é¡Œã«è½ã¨ã—è¾¼ã¿ï¼Œãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã¦æœ‰ç›Šãã†ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ´»ç”¨ã—ã¤ã¤ã€ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚‚æ¢ç´¢ã—ã¦ã„ãã€ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã¨ã£ãŸï¼æ‰‹æ³•ã¯ç·šå½¢å›å¸°ãƒ™ãƒ¼ã‚¹ã§æ·±å±¤å­¦ç¿’ã§ã¯ãªã„ï¼
### 51. [Pandian _et al._ (Scientific Reports, 2025), Enhancing lane detection in autonomous vehicles with multi-armed bandit ensemble learning](https://www.nature.com/articles/s41598-025-86743-z)
  - Awesome paper that introduces Multi-Armed Bandit Ensemble (MAB-Ensemble) for lane detection in autonomous vehicles, dynamically selecting optimal CNN models based on environmental conditions using Thompson sampling.
  - è‡ªå‹•é‹è»¢è»Šã®ãƒ¬ãƒ¼ãƒ³æ¤œå‡ºã«ãŠã„ã¦ï¼ŒThompson Samplingã‚’ç”¨ã„ã¦ç’°å¢ƒæ¡ä»¶ã«åŸºã¥ã„ã¦æœ€é©ãªCNNãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„é¸æŠã™ã‚‹Multi-Armed Bandit Ensemble (MAB-Ensemble)ã‚’ææ¡ˆã—ãŸè«–æ–‡ï¼

# ğŸ§° Toolsï¼ˆãƒ„ãƒ¼ãƒ«ï¼‰
## Awesome-Librariesï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
### 52. ğŸŒŸ[River](https://github.com/online-ml/river)
  - A Python library for online machine learning (with over 5k stars), covering time series forecasting, bandits, and so on.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ5000ã‚¹ã‚¿ãƒ¼è¶…ãˆï¼‰ã§ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆãªã©ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã€‚
### 53. [scikit-multiflow](https://scikit-multiflow.readthedocs.io/en/stable/index.html)
  - A machine learning library for streaming data in Python (~0.8k stars). Although it can handle drift detection and has a variety of algorithms other than neural networks, River is more common nowadays.
  - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸPythonå®Ÿè£…ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆç´„800ã‚¹ã‚¿ãƒ¼ï¼‰ï¼ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºæ©Ÿèƒ½ã‚’å‚™ãˆï¼Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä»¥å¤–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚‚è±Šå¯Œã ãŒï¼Œç¾åœ¨ã§ã¯Riverã«ä¸»æµãŒç§»ã£ãŸï¼
### 54. ğŸŒŸ[Vowpal Wabbit](https://vowpalwabbit.org/index.html)
  - A machine learning library implemented in various languages (with over 8,500 stars), primarily developed by Microsoft. It supports various learning paradigms, including online learning, and can handle online prediction-related stuff like contextual bandits."
  - MicrosoftãŒä¸­å¿ƒã¨ãªã£ã¦é–‹ç™ºã—ã¦ã„ã‚‹å¤šè¨€èªå®Ÿè£…ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆ8500ã‚¹ã‚¿ãƒ¼è¶…ãˆï¼‰ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’å«ã‚€å¤šæ§˜ãªå­¦ç¿’æ§˜å¼ã«å¯¾å¿œã—ã¦ãŠã‚Šï¼ŒContextual banditsç­‰ã‚’æ‰±ãˆã‚‹ï¼
### 55. [MOA](https://moa.cms.waikato.ac.nz/)
  - An open-source Java framework designed for sequential data processing, boasting over 600 stars.
  - JAVAã§å®Ÿè£…ã•ã‚ŒãŸï¼Œé€æ¬¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼600ã‚¹ã‚¿ãƒ¼è¶…ãˆï¼ 
### 56. [CapyMOA](https://capymoa.org/)
  - Python implementation of MOA, significantly faster than River and suited for real-time processing.
  - MOAã®Pythonç‰ˆï¼Riverã‚ˆã‚Šã‚‚å¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¦ãŠã‚Šï¼Œãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å‘ãï¼
### 57. [Jubatas](http://jubat.us/ja/index.html)
  - A distributed processing framework for online machine learning, jointly developed by PFN and NTT.
  - PFNã¨NTTãŒå…±åŒã§é–‹ç™ºã—ã¦ã„ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’å‘ã‘ã®åˆ†æ•£å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼
### 58. [Deep-River](https://online-ml.github.io/deep-river/)
  - A library suitable for online learning of deep learning models implemented in PyTorch. Same developers as River (online-ml)."
  - PyTorchã§å®Ÿè£…ã•ã‚ŒãŸæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«é©ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Riverã¨åŒã˜online-mlãŒé–‹ç™ºï¼

## Awesome-Probability Inequalities (ç¢ºç‡ä¸ç­‰å¼)
### 59. [Probability inequalities](https://probability.oer.math.uconn.edu/wp-content/uploads/sites/2187/2020/08/ch15M.pdf)
  - Introduction of several probability inequalities used in proofs.
  - è¨¼æ˜ã«ä½¿ç”¨ã•ã‚Œã‚‹ç¢ºç‡ä¸ç­‰å¼ãŒã„ãã¤ã‹ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ï¼
### 60. ğŸŒŸ[Markov's Inequality ... Made Easy!](https://www.youtube.com/watch?v=e-nAr3MkAII)
  - Awesome YouTube video on Markov's inequality.
  - ãƒãƒ«ã‚³ãƒ•ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 61. ğŸŒŸ[Chebyshev's Inequality ... Made Easy!](https://www.youtube.com/watch?v=mlelI1LA9o4)
  - Awesome YouTube video on Chebyshev's inequality.
  - ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 62. ğŸ—¾[ã€å¤§å­¦æ•°å­¦ã€‘ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã€ç¢ºç‡çµ±è¨ˆã€‘](https://www.youtube.com/watch?v=d-ugoDdXWrU)
  - ãƒ¨ãƒ“ãƒãƒªã«ã‚ˆã‚‹ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®ä¸ç­‰å¼ã«ã¤ã„ã¦ã®è§£èª¬å‹•ç”»ï¼
### 63. [What is the Chernoff Bound?](https://www.youtube.com/watch?v=WKUeBoQp2Uo)
  - Awesome YouTube video on Chernoff bound.
  - ãƒã‚§ãƒ«ãƒãƒ•é™ç•Œã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 64. [L 27 | Cauchy Schwarz Inequality | Probability & Statistics | Digital Communication](https://www.youtube.com/watch?v=14-JD5KiUz0)
  - Awesome YouTube video on Cauchy-Schwarz inequality.
  - ã‚³ãƒ¼ã‚·ãƒ¼ï¼ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
  - ãƒã‚§ãƒ«ãƒãƒ•é™ç•Œã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 65. [Jensen's Inequality](https://www.youtube.com/watch?v=u0_X2hX6DWE)
  - Awesome YouTube video on Jensen's inequality.
  - ã‚¤ã‚§ãƒ³ã‚»ãƒ³ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 66. [A Visual Introduction to Hoeffding's Inequality - Statistical Learning Theory](https://www.youtube.com/watch?v=lsYPC0MuLJA)
  - Awesome YouTube video visualizing the concept of Hoeffding's inequality.
  - ã¸ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ç­‰å¼ã«ã¤ã„ã¦è¦–è¦šçš„ã«è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 67. [Supplemental Lecture notes Hoeffdingâ€™s inequality](https://cs229.stanford.edu/extra-notes/hoeffding.pdf)
  - Lecture material of Stanford University, including an explanation of moment generating functions and a proof of Hoeffding's inequality.
  - ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¯é–¢æ•°ã®è§£èª¬ã‚„Hoeffdingã®ä¸ç­‰å¼ã®è¨¼æ˜ã‚’å«ã‚“ã ï¼Œã‚¹ã‚¿ãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰å¤§å­¦ã®è¬›ç¾©è³‡æ–™ï¼
### 68. ğŸ—¾[ãƒ˜ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ç­‰å¼(Hoeffding's inequality)ã¨è«¸ã€…ã®ç¢ºç‡ã®è©•ä¾¡ã®ä¸ç­‰å¼](https://ludu-vorton.hatenablog.com/entry/2019/06/06/073000)
  - çµ±è¨ˆçš„å­¦ç¿’ç†è«–ã§ç¢ºç‡ã®è©•ä¾¡ã§ç”¨ã„ã‚‰ã‚Œã‚‹æ§˜ã€…ãªä¸ç­‰å¼ï¼ˆã¸ãƒ•ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸ç­‰å¼ã‚’å«ã‚€ï¼‰ã«ã¤ã„ã¦ã®è§£èª¬ï¼

## Awesome-Convex Optimization (å‡¸æœ€é©åŒ–)
### 69. [Subgradients/Subderivatives - Convex Analysis](https://www.youtube.com/watch?v=o0rOaN5uo64)
  - Awesome YouTube video on subgradients and subderivatives.
  - åŠ£å‹¾é…/åŠ£å¾®åˆ†ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 70. [Lipschitz Continuity | Lipschitz Condition](https://www.youtube.com/watch?v=P-OFTp3BPis) 
  - Awesome YouTube video on Lipschitz continuity.
  - ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 71. ğŸ—¾[ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã¨ã¯ï½å®šç¾©ã¨æ€§è³ªãƒ»ä»–ã®é€£ç¶šæ€§ã¨ã®é–¢ä¿‚ãªã©ï½](https://mathlandscape.com/lipschitz/)
  - ãƒªãƒ—ã‚·ãƒƒãƒ„é€£ç¶šã®å®šç¾©ã‚„ä¾‹ï¼Œæ€§è³ªï¼Œãã®ä»–ã®é€£ç¶šæ€§ã¨ã®é–¢é€£æ€§ã«ã¤ã„ã¦è§£èª¬ã—ãŸè¨˜äº‹ï¼
### 72. [Lagrange Multipliers](https://www.youtube.com/watch?v=5-CUqogfPLY)
  - Awesome YouTube video on Lagrange multipliers.
  - ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 73. [Understanding Lagrange Multipliers Visually](https://www.youtube.com/watch?v=5A39Ht9Wcu0)
  - Awesome YouTube video visualizing the concept of Lagrange multipliers.
  - ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦è¦–è¦šçš„ã«è§£èª¬ã—ãŸYouTubeå‹•ç”»ï¼
### 74. ğŸ—¾[ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã®æ°—æŒã¡ã€æ¡ä»¶ä»˜ãæ¥µå€¤å•é¡Œã€‘](https://www.youtube.com/watch?v=vAwqZmwf4W8)
  - ãƒ¨ãƒ“ãƒãƒªã«ã‚ˆã‚‹ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦ã®è§£èª¬å‹•ç”»ï¼å›³å½¢çš„æ„å‘³ã«ã¤ã„ã¦ã®è§£èª¬ã‚’å«ã‚€ï¼
### 75. ğŸ—¾[åˆ¶ç´„ä»˜ãæœ€é©åŒ–å•é¡Œ(KKTæ¡ä»¶/ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•)](https://www.youtube.com/watch?v=bdWTCq98H5c)
  - ãƒ¨ãƒ“ãƒãƒªã«ã‚ˆã‚‹ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã¤ã„ã¦ã®è§£èª¬å‹•ç”»ï¼KKTæ¡ä»¶ï¼ˆä¸ç­‰å¼åˆ¶ç´„ã®å ´åˆã®è§£æ³•ï¼‰ã«ã¤ã„ã¦ã®è§£èª¬ã‚’å«ã‚€ï¼
### 76. ğŸ—¾[ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã¨ã¯ï½æ„å‘³ã¨è¨¼æ˜ï½](https://mathlandscape.com/lagrange-multiplier/)
  - ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã®æ„å‘³ï¼Œå®šç†ã¨ãã®è¨¼æ˜ã‚’è§£èª¬ã—ãŸè¨˜äº‹ï¼

## Awesome-Gradient Descent (å‹¾é…é™ä¸‹æ³•)
### 77. ğŸŒŸ[Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=zRN94_SPD4hrQUUI)
  - Awesome explanation of gradient descent in deep learning by 3Blue1Brown.
  - 3Blue1Brownã«ã‚ˆã‚‹ï¼Œæ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹å‹¾é…é™ä¸‹æ³•ã«ã¤ã„ã¦ã®è§£èª¬ï¼
### 78. [Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)](https://youtu.be/NE88eqLngkg?si=qSmU5hpaeYiUtEZw)
  - Awesome explanation of the various online learning methods used in deep learning.
  - æ·±å±¤å­¦ç¿’ã«ç”¨ã„ã‚‰ã‚Œã‚‹æ§˜ã€…ãªã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ‰‹æ³•ã«ã¤ã„ã¦ã®è§£èª¬ï¼
### 79. [Mini Batch Gradient Descent (C2W2L01)](https://www.youtube.com/watch?v=4qJaSmvhxi8)
  - Awesome YouTube video by Andrew Ng explaining mini-batch gradient descent.
  - Andrew Ngã«ã‚ˆã‚‹ï¼ŒãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”».
### 80. [Understanding Mini-Batch Gradient Descent (C2W2L02)](https://www.youtube.com/watch?v=-_4Zi8fCZO4)
  - The second YouTube video by Andrew Ng explaining mini-batch gradient descent.
  - Andrew Ngã«ã‚ˆã‚‹ï¼ŒãƒŸãƒ‹ãƒãƒƒãƒå‹¾é…é™ä¸‹æ³•ã«ã¤ã„ã¦è§£èª¬ã—ãŸYouTubeå‹•ç”»ã®ï¼’æœ¬ç›®.

# ğŸ“š Resourcesï¼ˆå­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ï¼‰
## Awesome-slidesï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰
### 81. ğŸŒŸ[Online Convex Optimization and Its Surprising Applications](https://groups.oist.jp/sites/default/files/imce/u129210/mlss/Lecture_slide/MLSS2024_Francesco_Orabona.pdf)
  - Awesome slides from MLSS2024 by Orabona on online convex optimization algorithms such as OGD and OMD, and their applications to other fields. The content is quite mathematical but beneficial.
  - MLSS2024ã«ãŠã‘ã‚‹ï¼ŒOrabonaã«ã‚ˆã‚‹OGDã‚„OMDã¨ã„ã£ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ãã®ä»–åˆ†é‡ã¸ã®å¿œç”¨ã«ã¤ã„ã¦ã®ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ã‹ãªã‚Šæ•°å­¦çš„ãªå†…å®¹ã ãŒæœ‰ç›Šï¼
### 82. [Online Learning Methods for Big Data Analytics](http://www.mysmu.edu.sg/faculty/chhoi/libol/icdm14tuto/index.html)
  - Awesome tutorial presented at IEEE ICDM2014.
  - IEEE ICDM2014ã§ç™ºè¡¨ã•ã‚ŒãŸãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¬›æ¼”ï¼
### 83. [Learning Methods for Online Prediction Problems](https://users.cecs.anu.edu.au/~ssanner/MLSS2010/Bartlett1.pdf)
  - Awesome lecture materials from UC Berkeley covering topics from the expert aggregation problem to online convex optimization, with applications such as portfolio optimization.
  - UC Berkeleyã«ãŠã‘ã‚‹è¬›ç¾©è³‡æ–™ï¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã‹ã‚‰ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ï¼Œå¿œç”¨ã¨ã—ã¦ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã¾ã§ã‚’æ‰±ã£ã¦ã„ã‚‹ï¼
### 84. [Follow the Leader: Theory and Applications](https://www.cs.ubc.ca/labs/lci/mlrg/slides/2019_summer_3_follow_the_leader.pdf)
  - Slides explaining Follow The Leader (FTL) and its derivative algorithms in online learning, along with their applications.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«ãŠã‘ã‚‹Follow The Leader (FTL) ã‚„ãã®æ´¾ç”Ÿã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼Œãã—ã¦ãã‚Œã‚‰ã®å¿œç”¨ã«ã¤ã„ã¦è§£èª¬ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ï¼
### 85. ğŸ—¾[å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ç†è«–ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ](https://ibisml.org/archive/ibis2014/ibis2014_bandit.pdf)
  - ç¢ºç‡çš„ï¼ŒãŠã‚ˆã³æ•µå¯¾çš„ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«ãŠã‘ã‚‹ï¼Œå ±é…¬æœ€å¤§åŒ–ï¼ˆãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ï¼‰ã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è§£èª¬ã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ï¼
### 86.ğŸ—¾[ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®ç†è«–ã¨å¿œç”¨](https://www.lab2.kuis.kyoto-u.ac.jp/keisan-genkai/reports/2005/zentai_1/04-takimoto.pdf)
  - æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€ï¼ˆå¾Œè¿°ï¼‰ã®è‘—è€…ã§ã‚‚ã‚ã‚‹ç€§æœ¬è‹±äºŒå…ˆç”Ÿã«ã‚ˆã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ãŠã‘ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã¨ãã®å¿œç”¨ä¾‹ã‚’å–ã‚Šä¸Šã’ã¦ã„ã‚‹ï¼

## Awesome-Textbooksï¼ˆæ›¸ç±ï¼‰
### 87. ğŸŒŸ[Prediction, Learning, and Games](https://www.cambridge.org/core/books/prediction-learning-and-games/A05C9F6ABC752FAB8954C885D0065C8F)
  - The bible on online learning, focusing on regret minimization and game-theoretic approaches to sequential decision-making
  - ãƒªã‚°ãƒ¬ãƒƒãƒˆæœ€å°åŒ–ã‚„ã‚²ãƒ¼ãƒ ç†è«–ã‚’é€šã˜ã¦é€æ¬¡æ„æ€æ±ºå®šå•é¡Œã‚’æ‰±ã†ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ãƒã‚¤ãƒ–ãƒ«çš„æ›¸ç±ï¼
### 88. [Introduction to Online Convex Optimization](https://sites.google.com/view/intro-oco/)
  - Awesome book by Hazan that covers a wide range of topics in the theory of online convex optimization.
  - Hazanã«ã‚ˆã‚‹ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ã®ç†è«–ã«é–¢ã™ã‚‹å¤šæ§˜ãªãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒãƒ¼ã—ãŸå…¥é–€æ›¸ï¼
### 89. ğŸŒŸğŸ—¾[æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚º ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€](https://www.kspub.co.jp/book/detail/1529229.html)
  - è‘—åãªæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã‚ˆã‚Šï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ã¦æ›¸ã‹ã‚ŒãŸä¸€å†Šï¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆçµ±åˆå•é¡Œã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¸æœ€é©åŒ–ãªã©ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®ä¸»è¦ãªå†…å®¹ã‚’ä¸€é€šã‚Šå­¦ã¹ã‚‹ï¼
### 90. ğŸ—¾[æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚º ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã€](https://www.kspub.co.jp/book/detail/1529038.html)
  - åŒã˜ãæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã‚ˆã‚Šï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã‚’å–ã‚Šä¸Šã’ãŸä¸€å†Šï¼ˆå‡ºç‰ˆã¯ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€ã‚ˆã‚Šã‚‚å‰ï¼‰ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã‚’å«ã‚€ï¼Œã‚ˆã‚Šåºƒç¯„ãªå†…å®¹ã‚’å­¦ã¹ã‚‹ï¼ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã€ã‚ˆã‚Šã‚‚å¹³æ˜“ï¼
### 91. ğŸ—¾[æ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚º ã€Œãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®ç†è«–ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€](https://www.kspub.co.jp/book/detail/1529175.html)
  - åŒã˜ãæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚·ãƒªãƒ¼ã‚ºã‚ˆã‚Šï¼ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«ãŠã‘ã‚‹ãƒªã‚°ãƒ¬ãƒƒãƒˆè§£æã‚„å¿œç”¨ä¾‹ãªã©ã‚’ã‚ˆã‚Šå°‚é–€çš„ã«æ‰±ã£ã¦ã„ã‚‹ï¼

## Videosï¼ˆå‹•ç”»ï¼‰
### 92. ğŸŒŸ[An introduction to regret analysis: environment models and best-of-both-worlds](https://youtu.be/pCER8iuTdR4?si=XAL6lP5tj0mMf8yp)
  - Awesome introduction on online learning, regret analysis, and best-of-both-worlds algorithms in the Machine Learning Summer School 2024.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼Œãƒªã‚°ãƒ¬ãƒƒãƒˆè§£æï¼ŒBest-of-both-worldsã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦ã®Machine Learning Summer School 2024ã§ã®è¬›æ¼”ï¼
### 93. [Predict with online prediction in Vertex AI](https://youtu.be/TEE7uUbXWDY?si=nHXMfZyTb13KpmWD)
  - Awesome tutorial on how to make predictions on tabular datasets with online prediction in Vertex AI. [GitHub Link](https://github.com/rafaello9472/c4ds/tree/main/Predict%20with%20online%20prediction%20in%20Vertex%20AI)
  - Googleã®Vertex AIã‚’ç”¨ã„ã¦è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã‚’è¡Œã†æ–¹æ³•ã«ã¤ã„ã¦ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼[GitHub Link](https://github.com/rafaello9472/c4ds/tree/main/Predict%20with%20online%20prediction%20in%20Vertex%20AI)
### 94. ["Online" prediction vs "batch" prediction in machine learning](https://youtu.be/DnmWTIeQ7PM?si=Mg8xcbWXyzlP3vLY)
  - Awesome explanation by Chip Huyen on the difference between online prediction and more common batch prediction and their applications.
  - Chip Huyenã«ã‚ˆã‚‹ï¼Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã¨é¦´æŸ“ã¿æ·±ã„ãƒãƒƒãƒäºˆæ¸¬ã¨ã®é•ã„ã‚„ãã‚Œã‚‰ã®å¿œç”¨ä¾‹ã«ã¤ã„ã¦ã®çŸ­ã„è§£èª¬ï¼
### 95. [ML Drift: Identifying Issues Before You Have a Problem](https://youtu.be/uOG685WFO00?si=7_ti70FDTD-B5tbO)
  - Awesome presentation by Amy Hodler on ML drifts and how to fix them. [Blog](https://www.fiddler.ai/blog/drift-in-machine-learning-how-to-identify-issues-before-you-have-a-problem)
  - Amy Hodlerã«ã‚ˆã‚‹ï¼ŒMLãƒ‰ãƒªãƒ•ãƒˆã‚„ãã®è§£æ±ºæ–¹æ³•ã«ã¤ã„ã¦ã®ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼[ãƒ–ãƒ­ã‚°](https://www.fiddler.ai/blog/drift-in-machine-learning-how-to-identify-issues-before-you-have-a-problem)

## Articlesï¼ˆè¨˜äº‹ï¼‰
### 96. [What is Online Machine Learning?](https://medium.com/value-stream-design/online-machine-learning-515556ff72c5)
  - A blog post explaining the concept of online machine learning.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã«ã¤ã„ã¦è§£èª¬ã—ãŸãƒ–ãƒ­ã‚°è¨˜äº‹ï¼
### 97. [Anomalies detection using River](https://medium.com/spikelab/anomalies-detection-using-river-398544d3536)
  - A blog post explaining anomaly detection using River, including practical code examples.
  - Riverã‚’ç”¨ã„ãŸç•°å¸¸æ¤œçŸ¥ã«ã¤ã„ã¦ï¼Œå®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã‚’äº¤ãˆã¦è§£èª¬ã—ãŸãƒ–ãƒ­ã‚°è¨˜äº‹ï¼
### 98. ğŸŒŸğŸ—¾[ç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€Œã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã€](https://www.ai-gakkai.or.jp/resource/my-bookmark/my-bookmark_vol30-no5/)
  - äººå·¥çŸ¥èƒ½å­¦ä¼šèªŒã®é€£è¼‰ã€Œç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚’ç‰¹é›†ã—ãŸéš›ã®è¨˜äº‹ï¼ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤ã‚„ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼Œé–¢é€£å­¦ä¼šï¼Œé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã¾ã¨ã‚ã¦ã„ã‚‹ï¼
### 99. ğŸ—¾[ç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€Œå¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã€](https://www.ai-gakkai.or.jp/resource/my-bookmark/my-bookmark_vol31-no5/)
  - åŒã˜ãã€Œç§ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€ã‚ˆã‚Šï¼Œå¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã‚’ç‰¹é›†ã—ãŸå›ã®è¨˜äº‹ï¼é€æ¬¡çš„ãªæ„æ€æ±ºå®šã§ã‚ã‚‹ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã«é–¢ã™ã‚‹ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚„é–¢é€£å­¦ä¼šï¼Œé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Œé‡è¦è«–æ–‡ã‚’ã¾ã¨ã‚ã¦ã„ã‚‹ï¼
### 100. [Deus Ex Machinaã€ŒOnline learning and online predictionï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã«ã¤ã„ã¦ï¼‰ã€](https://deus-ex-machina-ism.com/?p=17082)
  - Good for understanding the confusing differences between online learning and online prediction, and for gaining an overview of their respective scopes.
  - æ··åŒã—ã‚„ã™ã„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã®é•ã„ã‚’çŸ¥ã‚Šï¼Œãã‚Œãã‚Œã®ç¯„å›²ã‚’æ¦‚è¦³ã™ã‚‹ã®ã«è‰¯ã„ï¼
### 101. [Deus Ex Machinaã€ŒOverview of online forecasting technology and various applications and implementationsï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬æŠ€è¡“ã®æ¦‚è¦ã¨æ§˜ã€…ãªé©ç”¨äº‹ä¾‹ã¨å®Ÿè£…ä¾‹ï¼‰ã€](https://deus-ex-machina-ism.com/?p=53594)
  - It introduces algorithms, libraries, applications, and suitable books for learning used in online prediction.
  - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³äºˆæ¸¬ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼Œãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼Œå¿œç”¨ä¾‹ï¼Œå­¦ç¿’ã«é©ã—ãŸæ›¸ç±ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ï¼
